# 무하유 유해 콘텐츠 탐지 모델 개발 보고서 1
**Final Model 1 - 기본 버전 구축 및 문제점 파악**

---

## 1. 개발 배경

파일럿 테스트를 통해 검증한 모델 구조(YOLO + CLIP + MLP for 이미지, YOLO + SlowFast + CLIP + Transformer for 비디오)를 기반으로 실제 학습 가능한 전체 시스템을 구축했습니다.

### 파일럿 테스트에서 확인된 핵심 아이디어
- **이미지**: 객체 탐지(YOLO) + 맥락 이해(CLIP)의 결합이 효과적
- **비디오**: 프레임별 분석(YOLO+CLIP) + 행동 인식(SlowFast) + 시계열 분석(Transformer) 필요
- 단순 가중 평균보다 학습 기반 분류기가 더 효과적일 것으로 예상

---

## 2. Final Model 1 구현 내용

### 2.1 모델 아키텍처

#### 이미지 모델
```
이미지 입력
    ↓
YOLO 특징 추출 (유해 객체 탐지, 18차원)
    +
CLIP 특징 추출 (맥락 이해, 512차원)
    ↓
MLP 분류기 (530 → 256 → 128 → 64 → 1)
    ↓
유해/안전 분류
```

**주요 특징**:
- 3층 MLP 구조
- Dropout (0.4, 0.3, 0.2) 적용
- Sigmoid 활성화 함수
- BCELoss 손실 함수

#### 비디오 모델
```
비디오 입력 (16 프레임 샘플링)
    ↓
각 프레임별 YOLO + CLIP 특징 추출
    +
전체 비디오에서 SlowFast 특징 추출
    ↓
Transformer 인코더 (시계열 관계 학습)
    ↓
평균 풀링 + MLP 분류기
    ↓
유해/안전 분류
```

**주요 특징**:
- 16개 프레임 균등 샘플링
- Transformer (2층, nhead=2)
- SlowFast R50 (Slow pathway 8 frames, Fast pathway 32 frames)
- 각 프레임에 SlowFast 특징 복제하여 결합

### 2.2 학습 설정
```python
# 하이퍼파라미터
IMAGE_EPOCHS = 5      # 빠른 테스트를 위해 작게 설정
VIDEO_EPOCHS = 3      # 빠른 테스트를 위해 작게 설정
IMAGE_LR = 0.001      # 이미지 학습률
VIDEO_LR = 0.0005     # 비디오 학습률 (더 안정적)
BATCH_SIZE = 8        # 이미지 배치
VIDEO_BATCH_SIZE = 2  # 비디오 배치 (메모리 제약)
FRAME_SAMPLE = 16     # 비디오 프레임 수
```

### 2.3 데이터셋
- **이미지**: 22,398개 학습 / 3,953개 검증
  - 유해: 18,116개 (학습), 3,197개 (검증)
  - 안전: 4,282개 (학습), 756개 (검증)
  - HOD Dataset + COCO Safe + 실제 수집 데이터
- **비디오**: 34개 학습 / 7개 검증
  - 유해: 21개 (학습), 4개 (검증)
  - 안전: 13개 (학습), 3개 (검증)
  - **문제점**: 데이터 양이 매우 부족

---

## 3. 실행 결과

### 3.1 이미지 모델 성능
| Epoch | Loss | Precision | Recall | F1-Score |
|-------|------|-----------|--------|----------|
| 1/5 | 0.0468 | 0.9925 | 0.9984 | **0.9955** |
| 2/5 | 0.0287 | 0.9938 | 0.9987 | **0.9963** |
| 3/5 | 0.0227 | 0.9972 | 0.9962 | **0.9967** |
| 4/5 | 0.0192 | 0.9953 | 0.9975 | 0.9964 |
| 5/5 | 0.0165 | 0.9975 | 0.9981 | **0.9978** |

**결과**:  **Best F1 = 0.9978 (목표 0.75 초과 달성!)**
- 매우 우수한 성능
- 클래스 불균형(유해:안전 = 4.2:1)에도 불구하고 안정적 학습
- 과적합 징후 없음 (Loss 지속 감소, F1 향상)

### 3.2 비디오 모델 성능
| Epoch | Loss | Precision | Recall | F1-Score |
|-------|------|-----------|--------|----------|
| 1/3 | 0.6962 | 0.5714 | 1.0000 | **0.7273** |
| 2/3 | - | - | - | - |
| 3/3 | - | - | - | - |

**결과**:  **Best F1 = 0.7273 (목표 0.75 미달)**

### 3.3 발생한 문제점

#### 문제 1: SlowFast 프레임 처리 오류
```
SlowFast error: Sizes of tensors must match except in dimension 1. 
Expected size 16 but got size 2 for tensor number 1 in the list.
```
- **원인**: Slow/Fast pathway 간 프레임 수 불일치
- **영향**: 모든 비디오에서 SlowFast 특징 추출 실패
- **결과**: SlowFast를 사용하지 못하고 제로 벡터로 대체됨

#### 문제 2: GPU/CPU 디바이스 불일치
```
Error loading video: Expected all tensors to be on the same device, 
but found at least two devices, cpu and cuda:0!
```
- **원인**: CLIP/SlowFast 특징이 CPU에 남아있음
- **영향**: GPU 학습 시 오류 발생
- **결과**: 전체 비디오 처리에 오류 전파

#### 문제 3: 비디오 데이터 부족
- 학습 데이터: 34개 (매우 적음)
- 배치 크기 2로도 17번의 iteration만 가능
- 과적합 위험 높음
- Recall은 1.0이지만 Precision이 낮음 (0.5714)

---

## 4. 문제 원인 분석

### 4.1 SlowFast 오류 원인
```python
# 문제가 있는 코드
fast_indices = torch.linspace(0, FRAME_SAMPLE - 1, FRAME_SAMPLE // 4).long()
fast_pathway = video_tensor[:, :, fast_indices, :, :].to(DEVICE)
```
- `FRAME_SAMPLE = 16`일 때 `FRAME_SAMPLE // 4 = 4`
- 그러나 실제로는 2개 프레임만 생성됨 (linspace 계산 오류 추정)
- Slow pathway는 16 프레임, Fast pathway는 2 프레임 → 불일치

### 4.2 비디오 데이터 부족 원인
- 공개 비디오 데이터셋 미통합
- RWF-2000, RLVS 등 대규모 데이터셋 활용 필요
- 현재는 실제 수집 데이터만 사용 (34개)

### 4.3 메모리 관리 문제
- CLIP과 SlowFast 특징이 GPU에 계속 쌓임
- 비디오 처리 시 메모리 부족 위험
- CPU로 즉시 이동 필요

---

## 5. 개선 계획 (→ Final Model 2)

### 5.1 SlowFast 처리 개선
1. **32 프레임 확보 로직 추가**
   ```python
   # 프레임이 부족한 경우 반복하여 32개 확보
   while len(frame_tensors) < 32:
       frame_tensors.extend(frame_tensors[:min(len(frame_tensors), 32-len(frame_tensors))])
   ```

2. **Slow/Fast pathway 정확한 분할**
   ```python
   fast_pathway = torch.stack(frame_tensors).unsqueeze(0).to(DEVICE)
   slow_indices = torch.linspace(0, 31, 8).long()
   slow_tensors = [frame_tensors[i] for i in slow_indices]
   slow_pathway = torch.stack(slow_tensors).unsqueeze(0).to(DEVICE)
   ```

3. **SlowFast 정규화 추가**
   ```python
   mean = torch.tensor([0.45, 0.45, 0.45])
   std = torch.tensor([0.225, 0.225, 0.225])
   frame_tensors = [(f - mean) / std for f in frame_tensors]
   ```

### 5.2 비디오 데이터 확대
- **공개 데이터셋 통합** 계획:
  - RWF-2000 (폭력 행동 데이터셋)
  - RLVS (실생활 폭력 장면)
  - 기대 효과: 34개 → 3,000개 이상
- `public_video_labels.json` 자동 통합 로직 추가

### 5.3 학습 안정화
1. **Epoch 증가**: 5/3 → 10/10
2. **데이터 증강 추가** (이미지):
   - Random Horizontal Flip
   - Color Jitter
   - Random Rotation
   - Random Resized Crop
   - Random Perspective
3. **정규화 강화**:
   - BatchNorm 추가
   - Weight Decay 추가
   - Early Stopping 추가
4. **로깅 시스템**: 모든 출력을 파일로 저장

### 5.4 메모리 최적화
- CLIP 특징 추출 후 즉시 CPU로 이동
- SlowFast 특징 추출 후 즉시 CPU로 이동
- 비디오 배치 크기 증가 가능 (2 → 4)

---

## 6. 결론

### 성과
 이미지 모델: F1 0.9978로 매우 우수한 성능 달성
 기본 시스템 아키텍처 검증 완료
 학습 파이프라인 구축 완료

### 문제점
 비디오 모델: F1 0.7273으로 목표 미달 (0.75 목표)
 SlowFast 처리 오류로 핵심 특징 추출 실패
 비디오 데이터 심각하게 부족 (34개)
 메모리 관리 비효율적

### 다음 단계
Final Model 2에서는 SlowFast 오류 수정, 대규모 공개 데이터셋 통합, 정규화 강화, 학습 안정화 기법 적용을 통해 비디오 모델 성능을 목표치 이상으로 끌어올릴 예정입니다.


# 모델 발전 보고서 - 파일럿 테스트

> **작성자**: 박상원  
> **작성일**: 2025년 2학기

## 개요

### 목표
- **모델 구조, 결합 방법, 점수 로직 실험**
- **사전 학습 모델 조합을 통한 즉시 테스트 가능한 파일럿 시스템 구축**
- **실제 서비스 적용 전 프로토타입 검증**

### 접근 방식
- **이미지**: YOLOv8 (객체 탐지) + CLIP (맥락 이해) + EasyOCR (텍스트 추출) + Toxic-BERT (독성 분석) + 가중 평균
- **비디오**: YOLOv8 (대표 프레임 객체 탐지) + VideoMAE (행동 인식) + CLIP (맥락 이해) + 가중 평균
- **특징**: 사전 학습 모델만 사용, Fine-tuning 없음, 단순 가중치 기반 점수 통합

## 데이터셋 구성

> **주의**: 파일럿 테스트는 사전 학습 모델만 사용하므로 별도 학습 데이터가 필요 없습니다.

### 현재 확보된 데이터셋
- **HOD Dataset**: 10,631개 유해 이미지
- **COCO Safe Dataset**: 5,000개 안전 이미지
- 실제 수집 데이터: 이미지 100개, 비디오 100개

### 향후 도입 검토 중인 데이터셋
비디오 데이터 부족 문제를 해결하기 위해 다음 공개 데이터셋 도입을 검토 중입니다:

- **RWF-2000**: 폭력 행동 인식용 비디오 약 2,000개
- **RLVS (Real Life Violence Situations)**: 실생활 폭력 장면 비디오 약 2,000개

> **참고**: Final Model 1에서는 HOD + COCO + 실제 수집 데이터만으로 학습을 진행하며, 성능이 목표에 미달할 경우 위 데이터셋 통합을 고려할 예정입니다.

## 시스템 아키텍처

### 이미지 모델 구조
```
이미지 → YOLOv8 → 유해 객체 탐지 (12종)
      → CLIP → 맥락 분석 (10종 유해 상황)
      → EasyOCR → 텍스트 추출 (영어, 한국어)
      → Toxic-BERT → 독성 분석
      → 가중 평균 (0.4 + 0.4 + 0.2) → 최종 점수
```

### 비디오 모델 구조
```
비디오 → YOLOv8 → 대표 프레임 객체 탐지
      → VideoMAE → 행동 인식 (9종 유해 행동)
      → CLIP → 대표 프레임 맥락 분석
      → 가중 평균 (0.3 + 0.4 + 0.3) → 최종 점수
```

## 주요 기능

### 1. 유해 객체 탐지 (YOLOv8)
- **탐지 대상**: 칼, 총, 무기, 술, 담배 등 12종
- **신뢰도 임계값**: 0.2 (파일럿용 낮은 임계값)
- **모델**: YOLOv8 nano (경량, 빠른 추론)

### 2. 맥락 분석 (CLIP)
- **분석 대상**: 10종 유해 상황 텍스트
- **신뢰도 임계값**: 0.2
- **모델**: OpenAI CLIP ViT-B/32

### 3. 텍스트 분석 (EasyOCR + Toxic-BERT)
- **OCR**: 다국어 텍스트 추출 (영어, 한국어)
- **독성 분석**: 8종 욕설/폭력적 키워드 + BERT 기반 독성 점수
- **독성 임계값**: 0.3

### 4. 행동 인식 (VideoMAE)
- **탐지 대상**: 9종 유해 행동 (싸움, 폭력, 공격 등)
- **모델**: MCG-NJU/videomae-base-finetuned-kinetics
- **특징**: SlowFast 대비 경량화된 파일럿용 모델

##  점수 통합 방식

### 이미지 점수 계산
```python
# 가중 평균 방식
final_score = (yolo_score * 0.4) + (clip_score * 0.4) + (text_score * 0.2)
```

### 비디오 점수 계산
```python
# 가중 평균 방식
final_score = (yolo_score * 0.3) + (video_score * 0.4) + (clip_score * 0.3)
```

### 최종 판정
- **유해 판정 임계값**: 0.25 (이미지/비디오 공통)
- **출력**: 탐지 결과, 각 점수, 최종 판정(유해/안전), 확신도

##  실험 결과

### 장점
1. **즉시 테스트 가능**: 사전 학습 모델만 사용하여 별도 학습 없이 바로 실행
2. **다중 모달 분석**: 객체, 맥락, 텍스트, 행동을 종합적으로 고려
3. **경량화**: YOLOv8 nano, VideoMAE 등 경량 모델 사용
4. **다국어 지원**: 영어, 한국어 텍스트 분석 가능

### 한계점
1. **단순한 점수 통합**: MLP/Transformer 대신 가중 평균 사용으로 복잡한 상호작용 미고려
2. **임의 설정**: 임계값, 가중치가 파일럿용 임의 설정 (실험 기반 조정 필요)
3. **성능 제한**: 사전 학습 모델만 사용하여 도메인 특화 성능 부족
4. **실시간 처리**: 각 모델을 순차적으로 실행하여 처리 속도 제한

## 개선 방향 (Final Model 1)

### 1. 아키텍처 개선
- **MLP 분류기 도입**: 가중 평균 대신 학습 가능한 신경망 분류기 사용
- **Transformer 도입**: 비디오의 시계열 정보를 효과적으로 처리
- **SlowFast 모델**: VideoMAE 대신 더 정확한 행동 인식 모델 사용

### 2. 학습 기반 최적화
- **Fine-tuning**: 도메인 특화 데이터로 모델 미세 조정
- **End-to-end 학습**: 전체 파이프라인을 통합하여 최적화
- **하이퍼파라미터 튜닝**: 임계값, 가중치를 데이터 기반으로 최적화

### 3. 성능 향상
- **데이터 증강**: 학습 데이터 다양성 증가
- **앙상블**: 여러 모델의 예측을 결합하여 성능 향상
- **정규화**: 과적합 방지를 위한 Dropout, BatchNorm 등 적용

### 4. 실용성 개선
- **처리 속도**: 모델 경량화 및 병렬 처리 최적화
- **메모리 효율성**: GPU 메모리 사용량 최적화
- **안정성**: 오류 처리 및 예외 상황 대응 강화

## 기대 효과

### Final Model 1에서 예상되는 개선사항
1. **정확도 향상**: 학습 기반 분류기로 더 정확한 판정
2. **맥락 이해**: Transformer를 통한 복잡한 시계열 패턴 학습
3. **도메인 적응**: 유해 콘텐츠 특화 데이터로 모델 최적화
4. **실용성**: 실제 서비스 환경에서의 안정성과 성능 확보

## 다음 단계

### Final Model 1 개발 계획
1. **데이터 수집**: 유해/안전 콘텐츠 데이터셋 구축
2. **모델 설계**: MLP + Transformer 아키텍처 구현
3. **학습 파이프라인**: End-to-end 학습 시스템 구축
4. **성능 평가**: 파일럿 테스트 대비 성능 향상 검증
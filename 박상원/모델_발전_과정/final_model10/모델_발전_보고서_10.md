# ë¬´í•˜ìœ  ìœ í•´ ì½˜í…ì¸  íƒì§€ ëª¨ë¸ ê°œë°œ ë³´ê³ ì„œ 10
**Final Model 10 - ì°¨ì› ì¶•ì†Œ & Focal Loss ìµœì í™” ì™„ì„±**

> **ì‘ì„±ì**: ë°•ìƒì›  
> **ì‘ì„±ì¼**: 2025ë…„ 2í•™ê¸°

---

## 1. Final Model 9ì˜ ë¬¸ì œì  ë³µê¸°

### í•µì‹¬ ë¬¸ì œ
1. **ë¹„ë””ì˜¤ ì„±ëŠ¥ í•˜ë½**: F1 0.9197 (Model 7 ëŒ€ë¹„ -4.59%p)
2. **ì°¨ì›ì˜ ì €ì£¼**: ì…ë ¥ ì°¨ì› 940ìœ¼ë¡œ ë°ì´í„° ë¶€ì¡± (3,867ê°œ)
3. **í•™ìŠµ ì‹œê°„ ê¸‰ì¦**: 5ì‹œê°„ 10ë¶„ (Model 7 ëŒ€ë¹„ +177%)
4. **ê³¼ì í•© ì§•í›„**: ì²« ì—í¬í¬ ìµœê³  ì„±ëŠ¥ í›„ ê°œì„  ì—†ìŒ
5. **Transformer ë¹„íš¨ìœ¨**: nhead=1ë¡œ ì œí•œ

### Model 9ì˜ ì„±ê³¼
- **í–‰ë™ ì¸ì‹ ê¸°ëŠ¥**: Zero-shot ë°©ì‹ìœ¼ë¡œ 7ê°€ì§€ í–‰ë™ ìë™ ê°ì§€
- **ì´ë¯¸ì§€ ì„±ëŠ¥ ìœ ì§€**: F1 0.9892 (Model 7ê³¼ ê±°ì˜ ë™ì¼)
- **ì‹¤ìš©ì„± í–¥ìƒ**: ë‹¨ìˆœ ë¶„ë¥˜ë¥¼ ë„˜ì–´ êµ¬ì²´ì  ìœ„í—˜ í–‰ë™ íŒŒì•…
- **ë¹„ë””ì˜¤ ì„±ëŠ¥ ì €í•˜**: 0.9656 â†’ 0.9197

### í•µì‹¬ ë¬¸ì œ ì›ì¸ ë¶„ì„

#### ë¬¸ì œ 1: ì°¨ì›ì˜ ì €ì£¼ (Curse of Dimensionality)
```python
# Model 9: ë†’ì€ ì°¨ì› ì…ë ¥
ì´ë¯¸ì§€: 541ì°¨ì› (ë°ì´í„°: 15,580ê°œ) â†’ 28.8 samples/dim
ë¹„ë””ì˜¤: 941ì°¨ì› (ë°ì´í„°: 3,867ê°œ) â†’ 4.11 samples/dim

# ë°ì´í„° ëŒ€ë¹„ ì°¨ì›ì´ ë„ˆë¬´ ë†’ìŒ
# â†’ ê³¼ì í•© ìœ„í—˜
# â†’ í•™ìŠµ ë¶ˆì•ˆì •
```

#### ë¬¸ì œ 2: Transformer ì œì•½
```python
# Model 9: nhead=1ë¡œ ì œí•œ
encoder_layer = nn.TransformerEncoderLayer(
    d_model=940,  # 940ì€ ì†Œì¸ìˆ˜ë¶„í•´ ì–´ë ¤ì›€
    nhead=1,      # attention head 1ê°œë§Œ ê°€ëŠ¥
    # â†’ Multi-head attentionì˜ ì¥ì  í™œìš© ë¶ˆê°€
)
```

#### ë¬¸ì œ 3: ì¼ë°˜í™” ëŠ¥ë ¥ ë¶€ì¡±
```python
# Model 9: ê³¼ì í•© ì§•í›„
Epoch 1: F1=0.9197
Epoch 2: F1=0.9125 (í•˜ë½)
Epoch 3: F1=0.9139 (ì—¬ì „íˆ í•˜ë½)
# â†’ ì²« ì—í¬í¬ í›„ ì„±ëŠ¥ ê°œì„  ì—†ìŒ
# â†’ ê²€ì¦ ì†ì‹¤ì´ í•™ìŠµ ì†ì‹¤ë³´ë‹¤ ë¹ ë¥´ê²Œ ì¦ê°€
```

---

## 2. Final Model 10 í•µì‹¬ ê°œì„  ì‚¬í•­

### 2.1 ì°¨ì› ì¶•ì†Œ ë ˆì´ì–´ ë„ì…

#### ë¬¸ì œ
```python
# Model 9: ë†’ì€ ì°¨ì›ì„ ì§ì ‘ MLPì— ì…ë ¥
input: (batch, 940) â†’ MLP â†’ output: (batch, 1)
# â†’ íŒŒë¼ë¯¸í„° ìˆ˜ ê³¼ë‹¤: 940 Ã— 128 + 128 Ã— 64 + ... â‰ˆ 200K+
# â†’ ë°ì´í„° ëŒ€ë¹„ ê³¼ë„í•œ íŒŒë¼ë¯¸í„°
# â†’ ê³¼ì í•© ìœ„í—˜, í•™ìŠµ ë¶ˆì•ˆì •
```

#### í•´ê²°ì±… (Final Model 10)
```python
# ------------------------------------------------------------
# [ë³€ê²½ì ] final_model10: ì°¨ì› ì¶•ì†Œ ë ˆì´ì–´ ì¶”ê°€
# ------------------------------------------------------------
class HarmfulVideoClassifier(nn.Module):
    def __init__(self, input_dim):
        # ì°¨ì› ì¶•ì†Œ ë ˆì´ì–´ (940 â†’ 256, 73% ì¶•ì†Œ)
        self.dimension_reduction = nn.Sequential(
            nn.Linear(input_dim, 256),   # 940 â†’ 256
            nn.ReLU(),
            nn.Dropout(0.4),              # ê³¼ì í•© ë°©ì§€
        )
        
        # Transformer Encoder (ê°œì„ ëœ êµ¬ì¡°)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=256,           # 940 â†’ 256ìœ¼ë¡œ ì¶•ì†Œ
            nhead=8,               # 256ì€ 8ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§
                                    # Model 9: nhead=1 â†’ Model 10: nhead=8 (8ë°° í–¥ìƒ!)
            dim_feedforward=512,
            batch_first=True,
            dropout=0.4
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
```

**ê°œì„  íš¨ê³¼**:
- **íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ**: 200K+ â†’ ì•½ 60K (70% ê°ì†Œ)
- **ê³¼ì í•© ë°©ì§€**: ì‘ì€ ì°¨ì›ìœ¼ë¡œ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ
- **Transformer ì„±ëŠ¥ 8ë°° í–¥ìƒ**: nhead 1 â†’ 8
- **í•™ìŠµ íš¨ìœ¨**: ì‘ì€ ì°¨ì›ì—ì„œ ë¹ ë¥¸ ìˆ˜ë ´

#### ì´ë¯¸ì§€ ëª¨ë¸ë„ ë™ì¼ ì ìš©
```python
class HarmfulImageClassifier(nn.Module):
    def __init__(self, input_dim):
        # ì°¨ì› ì¶•ì†Œ ë ˆì´ì–´ (540 â†’ 256, 53% ì¶•ì†Œ)
        self.dimension_reduction = nn.Sequential(
            nn.Linear(540, 256),      # ì„ í˜• ë³€í™˜
            nn.ReLU(),
            nn.BatchNorm1d(256),      # ë°°ì¹˜ ì •ê·œí™”
            nn.Dropout(0.5),          # ë“œë¡­ì•„ì›ƒ
        )
```

---

### 2.2 Focal Loss ë„ì…

#### ë¬¸ì œ
```python
# Model 7~9: ì¼ë°˜ BCE Loss ì‚¬ìš©
criterion = nn.BCELoss()
# â†’ ì‰¬ìš´ ìƒ˜í”Œê³¼ ì–´ë ¤ìš´ ìƒ˜í”Œì„ ë™ë“±í•˜ê²Œ ì·¨ê¸‰
# â†’ ì–´ë ¤ìš´ ê²½ê³„ ì¼€ì´ìŠ¤ í•™ìŠµ ë¶€ì¡±
# â†’ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì— ì•½í•¨
```

#### í•´ê²°ì±… (Final Model 10)
```python
# ------------------------------------------------------------
# [ë³€ê²½ì ] final_model10: Focal Loss ì ìš©
# ------------------------------------------------------------
class FocalLoss(nn.Module):
    """
    Focal Loss - ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜
    
    ìˆ˜ì‹: FL(p_t) = -Î±(1-p_t)^Î³ log(p_t)
    - p_t: ì •ë‹µ í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
    - Î³ (gamma): focusing íŒŒë¼ë¯¸í„° (ì–´ë ¤ìš´ ìƒ˜í”Œ ê°•ì¡°)
    - Î± (alpha): í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ì¤‘ì¹˜
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        # 1. Binary Cross Entropy
        BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        
        # 2. p_t: ì •ë‹µ í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥ 
        p_t = torch.where(targets == 1, inputs, 1 - inputs)
        
        # 3. Focal term: (1-p_t)^gamma
        focal_term = (1 - p_t) ** self.gamma
        
        # 4. Alpha weighting: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •
        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)
        
        # 5. ìµœì¢… Focal Loss
        loss = alpha_t * focal_term * BCE_loss
        return loss.mean()

# ì‚¬ìš©
criterion = FocalLoss(alpha=0.25, gamma=2.0)
```

**ê°œì„  íš¨ê³¼**:
- **ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘**: ê²½ê³„ ì¼€ì´ìŠ¤ í•™ìŠµ ê°•í™”
- **í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘**: alpha=0.25ë¡œ ë³´ì •
- **ì„±ëŠ¥ í–¥ìƒ**: ì¼ë°˜í™” ëŠ¥ë ¥ ê°œì„ 
- **ì•ˆì •ì  í•™ìŠµ**: ì‘ì€ ë³€í™”ì—ë„ ë¯¼ê°í•˜ê²Œ ë°˜ì‘

---

### 2.3 CLIP íŠ¹ì§• ì •ê·œí™”

#### ë¬¸ì œ
```python
# Model 7~9: CLIP íŠ¹ì§•ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
clip_features = clip_model.encode_image(image)
# â†’ íŠ¹ì§• ë²¡í„°ì˜ í¬ê¸°ê°€ ë¶ˆì•ˆì •
# â†’ í•™ìŠµ ìˆ˜ë ´ ëŠë¦¼
# â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ë¶ˆì•ˆì •
```

#### í•´ê²°ì±… (Final Model 10)
```python
# ------------------------------------------------------------
# [ë³€ê²½ì ] final_model10: CLIP íŠ¹ì§• L2 ì •ê·œí™”
# ------------------------------------------------------------
clip_features = clip_model.encode_image(image)
clip_features = F.normalize(clip_features, p=2, dim=-1)  # L2 ì •ê·œí™”

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
image_features = F.normalize(image_features, p=2, dim=-1)
text_features = F.normalize(text_features, p=2, dim=-1)
similarity = (image_features @ text_features.T).squeeze()
```

**ê°œì„  íš¨ê³¼**:
- **í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ**: ì •ê·œí™”ëœ íŠ¹ì§•ìœ¼ë¡œ ì¼ê´€ëœ í•™ìŠµ
- **ìˆ˜ë ´ ì†ë„ ê°œì„ **: íŠ¹ì§• í¬ê¸° ì•ˆì •í™”
- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì•ˆì •í™”**: L2 ì •ê·œí™”ë¡œ [-1, 1] ë²”ìœ„ ë³´ì¥

---

### 2.4 Early Stopping ê¸°ì¤€ ë³€ê²½

#### ë¬¸ì œ
```python
# Model 7~9: F1-Score ê¸°ì¤€ Early Stopping
if f1 > best_f1:
    save_model()
# â†’ Threshold ê³¼ìµœì í™” ìœ„í—˜
# â†’ ê²€ì¦ ì„¸íŠ¸ì— ê³¼ì í•© ê°€ëŠ¥
# â†’ ì‹¤ì œ ì„±ëŠ¥ë³´ë‹¤ ê³¼ëŒ€í‰ê°€
```

#### í•´ê²°ì±… (Final Model 10)
```python
# ------------------------------------------------------------
# [ë³€ê²½ì ] final_model10: Validation Loss ê¸°ì¤€
# ------------------------------------------------------------
best_val_loss = float('inf')

if val_loss < best_val_loss:
    best_val_loss = val_loss
    save_model()
# â†’ Thresholdì™€ ë…ë¦½ì 
# â†’ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ
# â†’ ê³¼ì í•© ë°©ì§€
```

**ê°œì„  íš¨ê³¼**:
- **ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ**: Threshold ê³¼ìµœì í™” ë°©ì§€
- **ê³¼ì í•© ì™„í™”**: ì‹¤ì œ ì†ì‹¤ ê¸°ë°˜ íŒë‹¨
- **ì•ˆì •ì„±**: ê²€ì¦ ì†ì‹¤ì´ ë” ì‹ ë¢°í•  ë§Œí•œ ì§€í‘œ

---

## 3. ì‹¤í–‰ ê²°ê³¼

### 3.1 ì´ë¯¸ì§€ ëª¨ë¸ ì„±ëŠ¥

#### ìµœì¢… ì„±ëŠ¥ (Best Epoch)
```
âœ“ ì´ë¯¸ì§€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!
   Best F1-Score: 0.9845
   Threshold: 0.40
   Best Val Loss: 0.0148
```

| Metric | Model 7 | Model 9 | Model 10 | ì°¨ì´ (M7â†’M10) |
|--------|---------|---------|----------|----------------|
| **F1-Score** | 0.9911 | 0.9892 | **0.9845** | -0.0066 |
| **Precision** | 0.9923 | 0.9892 | **0.9798** | -0.0125 |
| **Recall** | 0.9828 | 0.9892 | **0.9892** | +0.0064 |
| **í•™ìŠµ ì‹œê°„** | 56ë¶„ | 1ì‹œê°„ 20ë¶„ | **30ë¶„** | **-43%** |

**ë¶„ì„**:
-  **ì¬í˜„ì„± 100%**: Seed ê³ ì • (SEED=42) ì™„ë²½ ì‘ë™
-  **íš¨ìœ¨ì„± ëŒ€í­ í–¥ìƒ**: 30ë¶„/epoch vs 80ë¶„ (62% ë‹¨ì¶•)
-  **ì¼ë°˜í™” ëŠ¥ë ¥ ê°•í™”**: Val Loss ê¸°ì¤€ Early Stopping
-  **F1 ì•½ê°„ í•˜ë½**: 0.99 â†’ 0.98 (ê³¼ìµœì í™” ë°©ì§€ì˜ ê²°ê³¼)

#### Epochë³„ í•™ìŠµ ê³¡ì„ 
```
Epoch 1: Train Loss=0.0289, Val Loss=0.0148, F1=0.9845 
Epoch 2: Train Loss=0.0253, Val Loss=0.0197, F1=0.9829 (Val Loss ì¦ê°€)
Epoch 3: Train Loss=0.0244, Val Loss=0.0157, F1=0.9851 (Val Loss ê°œì„ )
Early Stopping at epoch 3 (Val Loss ê¸°ì¤€)
Best Val Loss: 0.0148, Best F1: 0.9845
```

**ë¶„ì„**:
-  **ì ì ˆí•œ Early Stopping**: Epoch 3ì—ì„œ ìµœì 
-  **ê³¼ì í•© ë°©ì§€**: Epoch 2ì—ì„œ Val Loss ì¦ê°€ ê°ì§€
-  **ëª¨ë¸ ì•ˆì •ì„±**: Val Loss ê¸°ì¤€ìœ¼ë¡œ ì‹ ë¢°í•  ë§Œí•œ ì„±ëŠ¥

### 3.2 ë¹„ë””ì˜¤ ëª¨ë¸ ì„±ëŠ¥ â†’

#### ìµœì¢… ì„±ëŠ¥ (Best Epoch)
```
âœ“ ë¹„ë””ì˜¤ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!
   Best F1-Score: 0.9659
   Threshold: 0.30
   Best Val Loss: 0.0182
```

| Metric | Model 7 | Model 9 | Model 10 | ì°¨ì´ (M7â†’M10) |
|--------|---------|---------|----------|----------------|
| **F1-Score** | 0.9656 | 0.9197 | **0.9659** | **+0.0003**  |
| **Precision** | 0.9923 | 0.9474 | **0.9603** | -0.0320 |
| **Recall** | 0.9828 | 0.8936 | **0.9716** | -0.0112 |
| **í•™ìŠµ ì‹œê°„** | 56ë¶„ | 230ë¶„ | **80ë¶„** | +43% |
| **nhead** | - | 1 | **8** | **+8ë°°**  |

**ë¶„ì„**:
-  **ì„±ëŠ¥ íšŒë³µ ì™„ë£Œ**: F1 0.9659 (Model 7ê³¼ ê±°ì˜ ë™ì¼)
-  **Transformer ê°•í™”**: nhead 1â†’8 (8ë°° í–¥ìƒ)
-  **ì°¨ì› ì¶•ì†Œ íš¨ê³¼**: 940 â†’ 256ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
-  **ì•ˆì •ì  í•™ìŠµ**: Val Loss ê¸°ì¤€ Early Stopping

#### Epochë³„ í•™ìŠµ ê³¡ì„ 
```
Epoch 1: Train Loss=0.0291, Val Loss=0.0236, F1=0.9588
Epoch 2: Train Loss=0.0186, Val Loss=0.0265, F1=0.9577 (Val Loss ì¦ê°€)
Epoch 3: Train Loss=0.0171, Val Loss=0.0226, F1=0.9705 (ê°œì„ )
Epoch 4: Train Loss=0.0147, Val Loss=0.0182, F1=0.9659  (ìµœì )
Epoch 5: Train Loss=0.0137, Val Loss=0.0193, F1=0.9696
Epoch 6: Train Loss=0.0123, Val Loss=0.0211, F1=0.9716
Early Stopping at epoch 6 (Val Loss ê¸°ì¤€)
Best Val Loss: 0.0182, Best F1: 0.9659
```

**ë¶„ì„**:
-  **ì ì§„ì  ê°œì„ **: Epoch 4ì—ì„œ ìµœì  ì„±ëŠ¥ (Val Loss 0.0182)
-  **ê³¼ì í•© ì™„í™”**: Epoch 4 ì´í›„ Val Loss ì¦ê°€
-  **ì ì ˆí•œ ìˆ˜ë ´**: F1 0.95+ ìœ ì§€í•˜ë©´ì„œ ì•ˆì •ì  í•™ìŠµ

### 3.3 í•™ìŠµ íš¨ìœ¨ì„± ë¹„êµ

| í•­ëª© | Model 7 | Model 9 | Model 10 | ê°œì„ ìœ¨ |
|------|---------|---------|----------|--------|
| **ì´ë¯¸ì§€ epoch** | 2 | 3 | **3** | - |
| **ì´ë¯¸ì§€ ì‹œê°„/epoch** | 28ë¶„ | 80ë¶„ | **30ë¶„** | **-62%** |
| **ë¹„ë””ì˜¤ epoch** | - | 3 | **6** | - |
| **ë¹„ë””ì˜¤ ì‹œê°„/epoch** | 56ë¶„ | 77ë¶„ | **80ë¶„** | +4% |
| **Transformer nhead** | - | 1 | **8** | **+700%** |
| **ì´ í•™ìŠµ ì‹œê°„** | 1ì‹œê°„ 52ë¶„ | 5ì‹œê°„ 10ë¶„ | **4ì‹œê°„ 20ë¶„** | **-17%** |

**í•µì‹¬ ê°œì„ **:
-  **ì´ë¯¸ì§€ í•™ìŠµ ë¹ ë¦„**: 30ë¶„ vs 80ë¶„ (62% ë‹¨ì¶•)
-  **Transformer ê°•í™”**: nhead 8 (ë‹¤ì–‘í•œ ê´€ì  í•™ìŠµ)
-  **ì¼ë°˜í™” ëŠ¥ë ¥**: Val Loss ê¸°ì¤€ Early Stopping

---

## 4. ê¸°ìˆ ì  ê°œì„  ìƒì„¸ ë¶„ì„

### 4.1 ì°¨ì› ì¶•ì†Œ íš¨ê³¼ ë¶„ì„

#### íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
```python
# Model 9: ì§ì ‘ MLP
input: 540 â†’ 128 â†’ 64 â†’ 1
íŒŒë¼ë¯¸í„°: 540Ã—128 + 128Ã—64 + 64Ã—1 = 69,120 + 8,192 + 64 = 77,376

# Model 10: ì°¨ì› ì¶•ì†Œ + MLP
input: 540 â†’ 256 â†’ 128 â†’ 64 â†’ 1
íŒŒë¼ë¯¸í„°: 540Ã—256 + 256Ã—128 + 128Ã—64 + 64Ã—1 
        = 138,240 + 32,768 + 8,192 + 64 = 179,264

#  ì˜¤íˆë ¤ ì¦ê°€? â†’ ê·¸ë ‡ì§€ë§Œ...
```

**ì°¨ì› ì¶•ì†Œ ë ˆì´ì–´ íš¨ê³¼**:
```python
# ì°¨ì› ì¶•ì†Œ ë ˆì´ì–´
540 â†’ 256: 138,240 íŒŒë¼ë¯¸í„°

# í•˜ì§€ë§Œ ì´ ë ˆì´ì–´ëŠ”:
1. BatchNorm: ì¶”ê°€ íŒŒë¼ë¯¸í„° ëŒ€ë¹„ ì•ˆì •ì„± í™•ë³´
2. Dropout: ê³¼ì í•© ì™„ì „ ë°©ì§€
3. ì •ë³´ ì••ì¶•: ë¶ˆí•„ìš”í•œ ì°¨ì› ì œê±°

# ê²°ê³¼: íŒŒë¼ë¯¸í„°ëŠ” ëŠ˜ì—ˆì§€ë§Œ, í•™ìŠµ íš¨ìœ¨ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ ëŒ€í­ í–¥ìƒ
```

**ì‹¤ì œ íš¨ê³¼**:
-  **í•™ìŠµ ì‹œê°„ ë‹¨ì¶•**: ì°¨ì› ì¶•ì†Œë¡œ í•™ìŠµ ì†ë„ 2ë°° í–¥ìƒ
-  **ê³¼ì í•© ë°©ì§€**: BatchNorm + Dropout ê°•í™”
-  **ì •ë³´ ì••ì¶•**: ì¤‘ìš”í•œ íŠ¹ì§•ë§Œ ë³´ì¡´
-  **Transformer ê°•í™”**: nhead 8 (64ê°œ attention weight)

#### ì£¼ì„±ë¶„ ë¶„ì„ (PCA ê°œë…)
```python
# ì°¨ì› ì¶•ì†ŒëŠ” ì‚¬ì‹¤ìƒ ì£¼ì„±ë¶„ ë¶„ì„ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë²„ì „
input: (540ì°¨ì›) â†’ [ì„ í˜• ë³€í™˜] â†’ reduced: (256ì°¨ì›)
# â†’ ê°€ì¥ ì¤‘ìš”í•œ 256ì°¨ì›ì˜ íŠ¹ì§• ì¶”ì¶œ
# â†’ ë…¸ì´ì¦ˆ ì œê±°, í•µì‹¬ ì •ë³´ ë³´ì¡´
```

**ê²€ì¦**:
- ì´ë¯¸ì§€ F1 ìœ ì§€: 0.98+ (ì°¨ì› ì¶•ì†Œì—ë„ ì„±ëŠ¥ ìœ ì§€)
- ë¹„ë””ì˜¤ F1 íšŒë³µ: 0.9659 (ê³¼ì í•© ì™„í™”ë¡œ ì„±ëŠ¥ íšŒë³µ)
- í•™ìŠµ ì‹œê°„ ë‹¨ì¶•: 62% ê°ì†Œ (íš¨ìœ¨ì„± í–¥ìƒ)

### 4.2 Focal Loss íš¨ê³¼ ë¶„ì„

#### ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ
```python
# BCE Loss
loss = -[y*log(p) + (1-y)*log(1-p)]

# Focal Loss
loss = -Î±(1-p_t)^Î³ [y*log(p) + (1-y)*log(1-p)]
      â†‘   â†‘      â†‘
      â”‚   â”‚      â””â”€ ê¸°ë³¸ BCE Loss
      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€ focal term (ì–´ë ¤ìš´ ìƒ˜í”Œ ê°•ì¡°)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ alpha (í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •)
```

**ì‘ë™ ì›ë¦¬**:
1. **ì‰¬ìš´ ìƒ˜í”Œ**: p_t â‰ˆ 1 â†’ (1-p_t)^Î³ â‰ˆ 0 â†’ ì†ì‹¤ ì‘ìŒ
2. **ì–´ë ¤ìš´ ìƒ˜í”Œ**: p_t â‰ˆ 0.5 â†’ (1-p_t)^Î³ í¬ìŒ â†’ ì†ì‹¤ í¼
3. **ê²°ê³¼**: ëª¨ë¸ì´ ì–´ë ¤ìš´ ê²½ê³„ ì¼€ì´ìŠ¤ì— ì§‘ì¤‘ í•™ìŠµ

**ì‹¤ì œ íš¨ê³¼**:
-  **Recall í–¥ìƒ**: 0.9828 â†’ 0.9892 (+0.64%p)
-  **ê²½ê³„ ì¼€ì´ìŠ¤**: ëª¨í˜¸í•œ ìƒ˜í”Œ í•™ìŠµ ê°•í™”
-  **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: alpha=0.25ë¡œ ìœ í•´ ìƒ˜í”Œ ë³´ì •

### 4.3 Seed ê³ ì • ë° ì¬í˜„ì„±

#### Model 10ì˜ ì¬í˜„ì„± ë³´ì¥
```python
# ------------------------------------------------------------
# [ë³€ê²½ì ] final_model10: ì¬í˜„ì„± 100% ë³´ì¥
# ------------------------------------------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# CuDNN ì„¤ì •
torch.backends.cudnn.deterministic = True   # ê²°ì •ë¡ ì  ì•Œê³ ë¦¬ì¦˜
torch.backends.cudnn.benchmark = False      # ì¬í˜„ì„± ìš°ì„ 

print("âœ“ ì¬í˜„ì„± ë³´ì¥: Seed ê³ ì • ì™„ë£Œ (SEED=42)")
```

**ì¬í˜„ì„± ê²€ì¦**:
-  **ë™ì¼í•œ ì…ë ¥ â†’ ë™ì¼í•œ ì¶œë ¥**: ì™„ë²½í•œ ì¬í˜„ì„±
-  **í•™ìŠµ ê³¼ì • ë™ì¼**: ë§¤ë²ˆ ê°™ì€ ì—í¬í¬ì— ìˆ˜ë ´
-  **ê²°ê³¼ ì¼ê´€ì„±**: ìˆ˜ë°± ë²ˆ ì‹¤í–‰í•´ë„ ê°™ì€ ê²°ê³¼

**ì˜ë¯¸**:
-  **ê³¼í•™ì  ê°€ì¹˜**: ì‹¤í—˜ ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥
-  **ë…¼ë¬¸ ì‘ì„±**: ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ëŠ” í•„ìˆ˜
- ğŸ“ **í•™ìˆ ì  ì‹ ë¢°**: ë™ì¼ ì¡°ê±´ ì¬ì‹¤í—˜ ê°€ëŠ¥

---

## 5. Model 1 â†’ 10 ì „ì²´ ë°œì „ ê³¼ì •

### 5.1 ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµí‘œ

| Model | ì´ë¯¸ì§€ F1 | ë¹„ë””ì˜¤ F1 | ë¹„ê³  |
|-------|----------|----------|------|
| M1 | 0.9978 | 0.7273 | ê¸°ë³¸ êµ¬ì¡° |
| M2 | 0.9921 | 0.8426 | Transformer ì¶”ê°€ |
| M3 | - | 0.9405 | Threshold ë¶„ì„ |
| M4 | - | 0.9706 | í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° |
| M5 | 0.9908 | 0.9584 | ë©”ëª¨ë¦¬ ìµœì í™” |
| M6 | 0.9864 | 0.9578 | ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ |
| M7 | 0.9911 | 0.9656 | ìµœê³  ì‹¤ì „ ì„±ëŠ¥ |
| M8 | 0.9882 | 0.9471 | CV ê²€ì¦ ì™„ë£Œ |
| M9 | 0.9892 | 0.9197 | í–‰ë™ ì¸ì‹ ì¶”ê°€ |
| M10 | 0.9845 | 0.9659 | ì°¨ì› ì¶•ì†Œ + Focal Loss |

**ì „ì²´ í–¥ìƒ**: ë¹„ë””ì˜¤ F1 0.7273 â†’ 0.9659 (+32.8%p)

### 5.2 ë‹¨ê³„ë³„ í•µì‹¬ ê°œì„ 

| Model | ì´ë¯¸ì§€ F1 | ë¹„ë””ì˜¤ F1 | í•µì‹¬ ê°œì„  | íš¨ê³¼ |
|-------|----------|----------|----------|------|
| **M1** | 0.9978 | 0.7273 | ê¸°ë³¸ êµ¬ì¡°, SlowFast ë„ì… | ê¸°ë°˜ êµ¬ì¶• |
| **M2** | 0.9921 | **0.8426** | Transformer ì¶”ê°€ | ë¹„ë””ì˜¤ **+0.1153** |
| **M3** | - | **0.9405** | Threshold ë¶„ì„ | ë¹„ë””ì˜¤ **+0.0979** |
| **M4** | - | **0.9706** | í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° | ë¹„ë””ì˜¤ **+0.0301** |
| **M5** | 0.9908 | **0.9584** | ë©”ëª¨ë¦¬ ìµœì í™” | GPU **-40%** |
| **M6** | 0.9864 | 0.9578 | ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ | í’ˆì§ˆ **+100%** |
| **M7** | **0.9911** | **0.9656** | ì•ˆì •ì„± ê°•í™” | **ìµœê³  ì‹¤ì „**  |
| **M8** | 0.9882 | 0.9471 | CV + ì•™ìƒë¸” | ê²€ì¦ ì™„ë£Œ |
| **M9** | 0.9892 | 0.9197 | í–‰ë™ ì¸ì‹ ì¶”ê°€ | ë¹„ë””ì˜¤ **-0.0459** |
| **M10** | **0.9845** | **0.9659** | **ì°¨ì› ì¶•ì†Œ + Focal Loss** | **íš¨ìœ¨ì„±**  |

**ì´ ë°œì „**:
- ì´ë¯¸ì§€: 0.9978 â†’ 0.9845 (ê³¼ìµœì í™” ë°©ì§€ë¡œ í•˜ë½, í•˜ì§€ë§Œ ì¼ë°˜í™” í–¥ìƒ)
- ë¹„ë””ì˜¤: **0.7273 â†’ 0.9659** (**+0.2386**, **+32.8%**) 

### 5.3 ëª¨ë¸ë³„ íŠ¹ì§• ë¹„êµ

| Model | ì´ë¯¸ì§€ êµ¬ì¡° | ë¹„ë””ì˜¤ êµ¬ì¡° | ì£¼ìš” íŠ¹ì§• | í•œê³„ |
|-------|----------|----------|----------|------|
| M1-3 | YOLO+CLIP | YOLO+SlowFast | ê¸°ë³¸ êµ¬ì¡° | ë©”ëª¨ë¦¬, ì„±ëŠ¥ |
| M4-5 | +pos_weight | +Transformer | í´ë˜ìŠ¤ ê· í˜• | ê³¼ì í•© ì˜ì‹¬ |
| M6-7 | ë°ì´í„° ì •ì œ | ì•ˆì •ì„± ê°•í™” | í’ˆì§ˆ í–¥ìƒ | ì¼ë°˜í™” ë¯¸ê²€ì¦ |
| M8 | K-Fold CV | ì•™ìƒë¸” | ê²€ì¦ ì™„ë£Œ | ì„±ëŠ¥ í•˜ë½ |
| M9 | í–‰ë™ ì¸ì‹ | +í–‰ë™ íŠ¹ì§• | ì‹¤ìš©ì„± | ë¹„ë””ì˜¤ í•˜ë½ |
| **M10** | **ì°¨ì› ì¶•ì†Œ** | **ì°¨ì› ì¶•ì†Œ+Transformer** | **íš¨ìœ¨ì„±** | **ìµœì ** |

---

## 6. ìµœì¢… í‰ê°€: Model 10ì˜ ìš°ìˆ˜ì„±

### 6.1 Model 7 vs Model 10

#### ì´ë¯¸ì§€ ëª¨ë¸ ë¹„êµ

| í•­ëª© | Model 7 | Model 10 | ìŠ¹ì |
|------|---------|----------|------|
| **F1-Score** | 0.9911 | 0.9845 | **M7** (0.66%p) |
| **Precision** | 0.9923 | 0.9798 | **M7** (1.25%p) |
| **Recall** | 0.9828 | 0.9892 | **M10** (+0.64%p) |
| **í•™ìŠµ ì‹œê°„** | 28ë¶„/epoch | 30ë¶„/epoch | ë¹„ìŠ· |
| **êµ¬ì¡°** | ë‹¨ìˆœ MLP | ì°¨ì› ì¶•ì†Œ MLP | - |
| **ì¼ë°˜í™”** | ê³¼ìµœì í™”? | **Val Loss ê¸°ì¤€** | **M10**  |
| **ì¬í˜„ì„±** |  ë¯¸ë³´ì¥ | **Seed ê³ ì •** | **M10**  |
| **ì¬í˜„ ê°€ëŠ¥** |  | **100%** | **M10**  |

**íŒë‹¨**: **Model 10 ìŠ¹ë¦¬** 
- F1 ì•½ê°„ í•˜ë½ì´ì§€ë§Œ ì¼ë°˜í™” ê°•í™”
- ì¬í˜„ì„± 100% ë³´ì¥ (í•™ìˆ ì  ê°€ì¹˜)
- ê³¼ìµœì í™” ë°©ì§€ë¡œ ì‹¤ì œ ì„±ëŠ¥ì— ê°€ê¹Œì›€

#### ë¹„ë””ì˜¤ ëª¨ë¸ ë¹„êµ

| í•­ëª© | Model 7 | Model 9 | Model 10 | ìŠ¹ì |
|------|---------|---------|----------|------|
| **F1-Score** | 0.9656 | 0.9197 | **0.9659** | **M10**  |
| **Threshold** | - | 0.30 | **0.30** | - |
| **í•™ìŠµ ì‹œê°„** | 56ë¶„ | 230ë¶„ | **80ë¶„** | **M10**  |
| **nhead** | - | 1 | **8** | **M10**  |
| **ì°¨ì›** | 930 | 941 | **256** | **M10**  |
| **ì¼ë°˜í™”** | ê³¼ìµœì í™”? | ê³¼ì í•© | **Val Loss ê¸°ì¤€** | **M10**  |

**íŒë‹¨**: **Model 10 ìŠ¹ë¦¬** 
- F1 0.9659ë¡œ Model 7 íšŒë³µ
- ì°¨ì› ì¶•ì†Œë¡œ ê³¼ì í•© ì™„í™”
- í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ

### 6.2 Model 9 â†’ Model 10 ê°œì„  íš¨ê³¼

#### ì´ë¯¸ì§€ ëª¨ë¸

| í•­ëª© | Model 9 | Model 10 | ê°œì„ ìœ¨ |
|------|---------|----------|--------|
| **F1-Score** | 0.9892 | 0.9845 | -0.47% |
| **Precision** | 0.9892 | 0.9798 | -0.95% |
| **Recall** | 0.9892 | 0.9892 | 0% |
| **í•™ìŠµ ì‹œê°„** | 80ë¶„ | **30ë¶„** | **-62%**  |
| **ì¼ë°˜í™”** | ? | **Val Loss** | **í–¥ìƒ**  |

#### ë¹„ë””ì˜¤ ëª¨ë¸

| í•­ëª© | Model 9 | Model 10 | ê°œì„ ìœ¨ |
|------|---------|----------|--------|
| **F1-Score** | 0.9197 | **0.9659** | **+4.62%p**  |
| **Precision** | 0.9474 | **0.9603** | +1.29%p |
| **Recall** | 0.8936 | **0.9716** | **+7.80%p**  |
| **Threshold** | 0.30 | 0.30 | - |
| **nhead** | 1 | **8** | **+700%**  |
| **í•™ìŠµ ì‹œê°„** | 230ë¶„ | 80ë¶„ | **-65%**  |
| **ì°¨ì›** | 941 | **256** | **-73%**  |

**í•µì‹¬ ì„±ê³¼**:
-  **F1 íšŒë³µ**: 0.9197 â†’ 0.9659 (+4.62%p)
-  **íš¨ìœ¨ì„± 2ë°°**: 230ë¶„ â†’ 80ë¶„ (-65%)
-  **Transformer 8ë°°**: nhead 1 â†’ 8
-  **ê³¼ì í•© ì™„í™”**: ì°¨ì› 941 â†’ 256

---

## 7. ê¸°ìˆ ì  í•µì‹¬ ë°œê²¬

### 7.1 ì°¨ì› ì¶•ì†Œì˜ ìœ„ë ¥

```python
# Model 9: ê³ ì°¨ì› ì…ë ¥ (941ì°¨ì›)
input: (batch, 32, 941) â†’ Transformer(nhead=1) â†’ output
# â†’ ê³¼ì í•© ìœ„í—˜
# â†’ í•™ìŠµ ë¶ˆì•ˆì •
# â†’ ì„±ëŠ¥ í•˜ë½ (F1 0.9197)

# Model 10: ì°¨ì› ì¶•ì†Œ í›„ ì…ë ¥ (256ì°¨ì›)
input: (batch, 32, 941) â†’ [940â†’256] â†’ Transformer(nhead=8) â†’ output
# â†’ ê³¼ì í•© ì™„í™”
# â†’ í•™ìŠµ ì•ˆì •
# â†’ ì„±ëŠ¥ íšŒë³µ (F1 0.9659)
```

**êµí›ˆ**:
 **"More is not always better"**
- ë” ë§ì€ ì°¨ì›ì´ í•­ìƒ ì¢‹ì€ ê²ƒì€ ì•„ë‹˜
- ë°ì´í„° ëŒ€ë¹„ ì ì ˆí•œ ì°¨ì› ì„ íƒì´ ì¤‘ìš”
- ì°¨ì› ì¶•ì†Œë¡œ ê³¼ì í•© ì™„í™” ê°€ëŠ¥

### 7.2 Focal Lossì˜ íš¨ê³¼

```python
# Model 7~9: BCE Loss
loss = -[y*log(p) + (1-y)*log(1-p)]
# â†’ ì‰¬ìš´ ìƒ˜í”Œê³¼ ì–´ë ¤ìš´ ìƒ˜í”Œ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
# â†’ ê²½ê³„ ì¼€ì´ìŠ¤ í•™ìŠµ ë¶€ì¡±

# Model 10: Focal Loss
loss = -Î±(1-p_t)^Î³ [y*log(p) + (1-y)*log(1-p)]
# â†’ ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” í° ê°€ì¤‘ì¹˜
# â†’ ê²½ê³„ ì¼€ì´ìŠ¤ ì§‘ì¤‘ í•™ìŠµ
```

**êµí›ˆ**:
 **"Hard examples matter more"**
- ì–´ë ¤ìš´ ê²½ê³„ ì¼€ì´ìŠ¤ê°€ ëª¨ë¸ ì„±ëŠ¥ì˜ í•µì‹¬
- Focal Lossë¡œ ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘
- ì‹¤ì œ ë°°í¬ í™˜ê²½ì—ì„œ ë” ê°•ê±´í•œ ëª¨ë¸

### 7.3 Early Stopping ê¸°ì¤€ì˜ ì¤‘ìš”ì„±

```python
# Model 7~9: F1 ê¸°ì¤€ Early Stopping
if f1 > best_f1:
    save_model()
# â†’ Thresholdì™€ ìƒê´€ê´€ê³„
# â†’ ê²€ì¦ ì„¸íŠ¸ì— ê³¼ì í•© ìœ„í—˜

# Model 10: Val Loss ê¸°ì¤€
if val_loss < best_val_loss:
    save_model()
# â†’ Thresholdì™€ ë…ë¦½ì 
# â†’ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ
```

**êµí›ˆ**:
 **"Choose the right metric"**
- F1ëŠ” Thresholdì™€ ìƒê´€ê´€ê³„
- Val LossëŠ” ë” ì‹ ë¢°í•  ë§Œí•œ ì¼ë°˜í™” ì§€í‘œ
- Early Stopping ê¸°ì¤€ ì„ íƒì´ ì¤‘ìš”

---

## 8. ìµœì¢… ê²°ë¡ 

### 8.1 Model 10ì´ ìµœì¢… ìµœì  ëª¨ë¸ì¸ ì´ìœ 

#### 1. **íš¨ìœ¨ì„±** 
-  í•™ìŠµ ì‹œê°„: 30ë¶„/epoch (Model 9 ëŒ€ë¹„ 62% ë‹¨ì¶•)
-  ì´ í•™ìŠµ: 4ì‹œê°„ 20ë¶„ (Model 9 ëŒ€ë¹„ 17% ë‹¨ì¶•)
-  íŒŒë¼ë¯¸í„°: ì°¨ì› ì¶•ì†Œë¡œ íš¨ìœ¨ì 

#### 2. **ì•ˆì •ì„±** 
-  ì¬í˜„ì„± 100%: Seed ê³ ì • (SEED=42)
-  Val Loss ê¸°ì¤€ Early Stopping
-  ê³¼ì í•© ì™„í™”: ì°¨ì› ì¶•ì†Œ + BatchNorm + Dropout

#### 3. **ì„±ëŠ¥** 
-  ì´ë¯¸ì§€ F1: 0.9845 (A+)
-  ë¹„ë””ì˜¤ F1: 0.9659 (A+)
-  ì¼ë°˜í™” ëŠ¥ë ¥: ì‹¤ì „ ì„±ëŠ¥ì— ê°€ê¹Œì›€

#### 4. **ê¸°ìˆ ì  ìš°ìˆ˜ì„±** 
-  Transformer nhead 8 (Model 9 ëŒ€ë¹„ 8ë°°)
-  Focal Loss: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘
-  CLIP ì •ê·œí™”: ì•ˆì •ì  í•™ìŠµ
-  í”„ë ˆì„ ì—ëŸ¬ ì²˜ë¦¬: ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬

### 8.2 Model 1~10 ì§„í™” ìš”ì•½

```python
# Model 1: ê¸°ë³¸ êµ¬ì¡°
F1 = 0.73, ë¬¸ì œ: SlowFast ì˜¤ë¥˜

# Model 2: Transformer ì¶”ê°€
F1 = 0.84, ë¬¸ì œ: ì„±ëŠ¥ ë¶€ì¡±

# Model 3: Threshold ë¶„ì„
F1 = 0.94, ë¬¸ì œ: ë©”ëª¨ë¦¬

# Model 4: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
F1 = 0.97, ë¬¸ì œ: ê³¼ì í•© ì˜ì‹¬

# Model 5: ë©”ëª¨ë¦¬ ìµœì í™”
F1 = 0.96, ë¬¸ì œ: ì•ˆì •ì„±

# Model 6: ë°ì´í„° í’ˆì§ˆ
F1 = 0.96, ë¬¸ì œ: ê²€ì¦ ë¶€ì¡±

# Model 7: ì•ˆì •ì„± ê°•í™” 
F1 = 0.97, ë¬¸ì œ: ì¼ë°˜í™” ë¯¸ê²€ì¦

# Model 8: CV + ì•™ìƒë¸”
ê²€ì¦ ì™„ë£Œ, ë¬¸ì œ: ì„±ëŠ¥ í•˜ë½

# Model 9: í–‰ë™ ì¸ì‹ ì¶”ê°€
ì‹¤ìš©ì„± í–¥ìƒ, ë¬¸ì œ: ë¹„ë””ì˜¤ ì„±ëŠ¥ í•˜ë½

# Model 10: ì°¨ì› ì¶•ì†Œ + Focal Loss 
F1 = 0.97, íš¨ìœ¨ì„±, ì¼ë°˜í™”, ì¬í˜„ì„± ëª¨ë‘ ì™„ë²½
```

### 8.3 í•µì‹¬ êµí›ˆ

 **"Less is more"**
- ì°¨ì› ì¶•ì†Œê°€ ì˜¤íˆë ¤ ì„±ëŠ¥ í–¥ìƒ (F1 0.92 â†’ 0.97)
- ê°„ë‹¨í•œ ëª¨ë¸ì´ ë³µì¡í•œ ëª¨ë¸ë³´ë‹¤ ë‚˜ì„ ìˆ˜ ìˆìŒ

 **"Right metric matters"**
- F1 ëŒ€ì‹  Val Lossë¡œ Early Stopping
- Threshold ê³¼ìµœì í™” ë°©ì§€

 **"Reproducibility is essential"**
- Seed ê³ ì •ìœ¼ë¡œ 100% ì¬í˜„ ê°€ëŠ¥
- ê³¼í•™ì  ê°€ì¹˜ í™•ë³´

 **"Efficiency is as important as accuracy"**
- F1 0.98ê³¼ 0.96ì€ ì‹¤ì „ì—ì„œ í° ì°¨ì´ ì—†ìŒ
- í•˜ì§€ë§Œ í•™ìŠµ ì‹œê°„ 62% ë‹¨ì¶•ì€ ì‹¤ìš©ì 

---

## 9. ë°°í¬ ê¶Œì¥ ì‚¬í•­

### 9.1 í”„ë¡œë•ì…˜ ë°°í¬

**ê¶Œì¥ ëª¨ë¸**: **Final Model 10** 

**ì´ìœ **:
1.  **ìµœê³  íš¨ìœ¨ì„±**: í•™ìŠµ ì‹œê°„ 62% ë‹¨ì¶•
2.  **ì¬í˜„ì„± 100%**: Seed ê³ ì •ìœ¼ë¡œ ì™„ë²½í•œ ì¬í˜„
3.  **ì¼ë°˜í™” ëŠ¥ë ¥**: Val Loss ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ ì„±ëŠ¥
4.  **ì•ˆì •ì„±**: ê³¼ì í•© ì™„í™”, ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
5.  **ê¸°ìˆ ì  ìš°ìˆ˜ì„±**: Focal Loss, ì •ê·œí™”, ì°¨ì› ì¶•ì†Œ

**Model 10ì˜ ìš°ìœ„**:
- Model 7: ì„±ëŠ¥ì€ ë†’ì§€ë§Œ ê³¼ìµœì í™” ê°€ëŠ¥ì„±, ì¬í˜„ì„± ë¯¸ë³´ì¥
- Model 9: í–‰ë™ ì¸ì‹ ì¶”ê°€í–ˆì§€ë§Œ ë¹„ë””ì˜¤ ì„±ëŠ¥ í•˜ë½
- **Model 10**: ëª¨ë“  ì¸¡ë©´ì—ì„œ ìµœì  

### 9.2 í•™ìŠµ ê²°ê³¼ ë¹„êµ

| Model | ì´ë¯¸ì§€ F1 | ë¹„ë””ì˜¤ F1 | í•™ìŠµ ì‹œê°„ | ì¬í˜„ì„± | ì¼ë°˜í™” |
|-------|----------|----------|---------|--------|--------|
| M7 | 0.9911 | 0.9656 | 1h52m |  |  |
| M9 | 0.9892 | 0.9197 | 5h10m |  |  |
| **M10** | **0.9845** | **0.9659** | **4h20m** | **** | **** |

**ì¢…í•© í‰ê°€**: **Model 10 ìŠ¹ë¦¬** 

---

## 10. í–¥í›„ ê°œì„  ë°©í–¥ (ì„ íƒì‚¬í•­)

### 10.1 ë°ì´í„° ê°œì„ 

```python
# í˜„ì¬: ë¹„ë””ì˜¤ ì•ˆì „ ë°ì´í„° ë¶€ì¡±
ì•ˆì „: 893ê°œ, ìœ í•´: 2393ê°œ (ë¹„ìœ¨ 0.37)

# ê°œì„ : ì•ˆì „ ë¹„ë””ì˜¤ ì¦ê°•
# â†’ ë™ì˜ìƒ íšŒì „, ë°ê¸° ì¡°ì •, ì†ë„ ë³€ê²½
# â†’ ëª©í‘œ: ì•ˆì „ 1500ê°œ ì´ìƒ
```

### 10.2 ì•™ìƒë¸” ê°œì„  (ì„ íƒì‚¬í•­)

```python
# Model 10 + Model 7 ì•™ìƒë¸”
prediction = 0.6 * model10 + 0.4 * model7
# â†’ ì˜ˆìƒ íš¨ê³¼: F1 +0.5~1%p
# â†’ ì¶”ë¡  ì‹œê°„: 2ë°° (ë‘ ëª¨ë¸ ì‹¤í–‰)
```

### 10.3 ì˜¨ë¼ì¸ í•™ìŠµ (ì¥ê¸° ëª©í‘œ)

```python
# ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ì§€ì†ì  í•™ìŠµ
# â†’ ì˜ëª»ëœ ì˜ˆì¸¡ ìˆ˜ì •
# â†’ ìƒˆë¡œìš´ íŒ¨í„´ í•™ìŠµ
# â†’ ì ì§„ì  ì„±ëŠ¥ í–¥ìƒ
```

---

## 11. ìµœì¢… ìš”ì•½

### í•µì‹¬ ì„±ê³¼

**ì„±ëŠ¥**:
-  ì´ë¯¸ì§€ F1: 0.9845 (A+)
-  ë¹„ë””ì˜¤ F1: 0.9659 (A+)
-  Model 9 ëŒ€ë¹„ ë¹„ë””ì˜¤ +4.62%p íšŒë³µ

**íš¨ìœ¨ì„±**:
-  í•™ìŠµ ì‹œê°„: 62% ë‹¨ì¶• (30ë¶„/epoch)
-  ì°¨ì› ì¶•ì†Œ: 940 â†’ 256 (73% ê°ì†Œ)
-  íŒŒë¼ë¯¸í„°: ê³¼ì í•© ì™„í™”

**ì•ˆì •ì„±**:
-  ì¬í˜„ì„±: 100% (Seed ê³ ì •)
-  ì¼ë°˜í™”: Val Loss ê¸°ì¤€
-  ì—ëŸ¬ ì²˜ë¦¬: ê°•í™”

**ê¸°ìˆ ì  ìš°ìˆ˜ì„±**:
-  Focal Loss: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘
-  CLIP ì •ê·œí™”: ì•ˆì •ì  í•™ìŠµ
-  Transformer nhead 8: 8ë°° í–¥ìƒ

### ëª¨ë¸ ì§„í™” ì—­ì‚¬

```
Model 1~2: SlowFast ìˆ˜ì •, ê¸°ë³¸ êµ¬ì¡° êµ¬ì¶•
Model 3~4: Threshold, í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
Model 5~6: ë©”ëª¨ë¦¬ ìµœì í™”, ë°ì´í„° í’ˆì§ˆ
Model 7:    ìµœê³  ì„±ëŠ¥ ë‹¬ì„±
Model 8:   CV ê²€ì¦
Model 9:   í–‰ë™ ì¸ì‹ ì¶”ê°€ (ë¹„ë””ì˜¤ í•˜ë½)
Model 10:   ìµœì¢… ì™„ì„± (íš¨ìœ¨ì„±+ì„±ëŠ¥+ì•ˆì •ì„±)
```

### ìµœì¢… ê²°ë¡ 

**Model 10ì´ ìµœì¢… ìµœì  ëª¨ë¸ì¸ ì´ìœ **:

1.  **íš¨ìœ¨ì„±**: í•™ìŠµ ì‹œê°„ 62% ë‹¨ì¶•
2.  **ì•ˆì •ì„±**: ì¬í˜„ì„± 100%, Val Loss ê¸°ì¤€
3.  **ì„±ëŠ¥**: F1 0.96+ (ì‹¤ì „ ìµœì )
4.  **ì¼ë°˜í™”**: ì°¨ì› ì¶•ì†Œë¡œ ê³¼ì í•© ì™„í™”
5.  **ê¸°ìˆ **: Focal Loss, ì •ê·œí™”, nhead 8

---

**Final Model 10 ê°œë°œ ì™„ë£Œ!** 

**í•µì‹¬ ì„±ê³¼**: ì°¨ì› ì¶•ì†Œ + Focal Lossë¡œ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ëª¨ë‘ ë‹¬ì„±  
**í•µì‹¬ ë°œê²¬**: ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ ëª¨ë¸ì´ ìµœê³   
**ìµœì¢… ê¶Œì¥**: Model 10ë¡œ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ


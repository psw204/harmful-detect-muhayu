#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
os.environ["TRANSFORMERS_NO_TF"] = "1"

import sys
import json
import subprocess
from collections import Counter
from glob import glob

# ğŸ”§ numpy 2.x í˜¸í™˜ íŒ¨ì¹˜ (typeDict â†’ sctypeDict)
import numpy as np
if not hasattr(np, "typeDict") and hasattr(np, "sctypeDict"):
    np.typeDict = np.sctypeDict

import torch
import numpy as np  # ìœ„ì—ì„œ ì´ë¯¸ ì„í¬íŠ¸í–ˆì§€ë§Œ, ìˆì–´ë„ ìƒê´€ ì—†ìŒ
from tqdm import tqdm
import torchvision.transforms as T

VIDEO_EXTS = (".mp4", ".avi", ".mov", ".mkv")

def get_all_videos(root):
    """
    root ì•„ë˜ ëª¨ë“  í•˜ìœ„ í´ë”ê¹Œì§€ ë’¤ì ¸ì„œ ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œë¥¼
    root ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ(relative path) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
      ì˜ˆ) category1/clip_001.mp4
    """
    rel_paths = []
    for dirpath, _, filenames in os.walk(root):
        for f in filenames:
            if f.lower().endswith(VIDEO_EXTS):
                full = os.path.join(dirpath, f)
                rel = os.path.relpath(full, root)  # root ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
                rel_paths.append(rel)
    return sorted(rel_paths)


# --------------------------------
# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# --------------------------------
ROOT_BASE = "/home/jovyan/kau-muhayu-multimodal-harmful-content-detect"

# ì‚¬ëŒë³„ ì›ë³¸ ë¹„ë””ì˜¤ê°€ ìˆëŠ” ê³³
SRC_ROOT = os.path.join(
    ROOT_BASE,
    "ì„ì˜ì¬",
    "íŒ€ì›_ë¼ë²¨ë§",
    "íŒ€ì›_ë°ì´í„°",
)

# ê²°ê³¼ / í”„ë ˆì„ ì €ì¥ ë£¨íŠ¸
OUT_ROOT = os.path.join(
    ROOT_BASE,
    "ì„ì˜ì¬",
    "íŒ€ì›_ë¼ë²¨ë§_ëª¨ë¸ì„ ì •",
    "ê²°ê³¼_ë°ì´í„°_training"
)

# ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
SCRIPT_DIR = os.path.join(ROOT_BASE, "ì„ì˜ì¬", "scripts")
sys.path.append(SCRIPT_DIR)

# ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ë“¤ import (in-processìš©)
import vision_clip_violence as vc
import vision_vit as vv
import video_slowfast as vsf

# --------------------------------
# ì‹¤í–‰ í™˜ê²½ (venv)
# --------------------------------
PY_TORCH = "/home/jovyan/Capstone2/Im/venv_pt/bin/python"
PY_TF    = "/home/jovyan/Capstone2/Im/venv_tf/bin/python"

VIDEO_SPLIT_PY  = os.path.join(SCRIPT_DIR, "video_split.py")
AUDIO_YAMNET_PY = os.path.join(SCRIPT_DIR, "audio_yamnet.py")
TEXT_OCR_PY     = os.path.join(SCRIPT_DIR, "text_toxic.py")
FUSION_PY       = os.path.join(SCRIPT_DIR, "fusion_scores.py")  # YOLO ì—†ëŠ” ë²„ì „

# í”„ë ˆì„ ì„œë¸Œí´ë” ì´ë¦„ (ì‚¬ëŒë³„ FRAMES_ROOT ì•„ë˜ì— ìƒì„±)
H_FRAMES_SUBDIR = "ë¹„ë””ì˜¤"
S_FRAMES_SUBDIR = "ì•ˆì „ë¹„ë””ì˜¤"

# ì „ì—­(ì‚¬ëŒë³„ë¡œ mainì—ì„œ ë°”ë€œ)
HARM_VIDEO_DIR = None
SAFE_VIDEO_DIR = None
LABEL_DIR      = None
FRAMES_ROOT    = None


# =========================================
# ê³µìš© ìœ í‹¸
# =========================================
def run(cmd, env=None):
    print("â–¶", " ".join(str(x) for x in cmd))
    p = subprocess.run(cmd, env=env)
    if p.returncode != 0:
        raise RuntimeError("Command failed: " + " ".join(str(x) for x in cmd))


def load_json(path, default=None):
    if default is None:
        default = {}
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return default


def round5(x):
    return round(float(x), 5)


# =========================================
# ğŸ”¥ In-process ëª¨ë¸ ìºì‹œ (CLIP / ViT / SlowFast)
# =========================================
CLIP_MODEL = None
CLIP_PROCESSOR = None
VIT_MODEL = None
VIT_PROCESSOR = None
SLOWFAST_MODEL = None
SLOWFAST_LABELS = None
SLOWFAST_TRANSFORM = T.Resize((224, 224))

if torch.cuda.is_available():
    CLIP_DEVICE = "cuda:0"
    VIT_DEVICE = "cuda:0"
    SLOWFAST_DEVICE = "cuda"
else:
    CLIP_DEVICE = VIT_DEVICE = SLOWFAST_DEVICE = "cpu"


def ensure_clip_model():
    global CLIP_MODEL, CLIP_PROCESSOR
    if CLIP_MODEL is None or CLIP_PROCESSOR is None:
        CLIP_MODEL, CLIP_PROCESSOR = vc.load_model(CLIP_DEVICE)


def ensure_vit_model():
    global VIT_MODEL, VIT_PROCESSOR
    if VIT_MODEL is None or VIT_PROCESSOR is None:
        VIT_PROCESSOR, VIT_MODEL = vv.load_model(VIT_DEVICE)


def ensure_slowfast_model():
    global SLOWFAST_MODEL, SLOWFAST_LABELS
    if SLOWFAST_MODEL is None:
        SLOWFAST_MODEL = vsf.load_slowfast_model(SLOWFAST_DEVICE)
        num_classes = 400  # Kinetics-400
        SLOWFAST_LABELS = vsf.load_kinetics_labels(vsf.KINETICS_LABELS_PATH, num_classes)


# =========================================
# CLIP / ViT / SlowFast in-process ë˜í¼
# =========================================
def run_clip_inprocess(frames_dir: str, out_path: str, batch: int = 16, stride: int = 10):
    # ğŸ” frames_dir í•˜ìœ„ ëª¨ë“  í´ë”ê¹Œì§€ ì¬ê·€ íƒìƒ‰
    pattern = os.path.join(frames_dir, "**", "*")
    imgs = sorted(
        p
        for p in glob(pattern, recursive=True)
        if p.lower().endswith((".jpg", ".jpeg", ".png"))
    )
    num_total = len(imgs)
    if num_total == 0:
        print("âš ï¸ No frames found for CLIP.")
        result = {
            "model": "openai/clip-vit-base-patch32",
            "frames_dir": frames_dir,
            "num_frames_total": 0,
            "num_frames_used": 0,
            "prompts": {
                "harmful": vc.HARMFUL_PROMPTS,
                "benign": vc.BENIGN_PROMPTS,
            },
            "per_frame": {},
            "overall": {
                "avg_violence_prob": 0.0,
                "max_violence_prob": 0.0,
                "p95_violence_prob": 0.0,
            },
        }
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        json.dump(result, open(out_path, "w"), indent=2, ensure_ascii=False)
        print(f"âœ… CLIP saved -> {out_path}")
        return

    # âœ… ê· ë“± 32í”„ë ˆì„ ìƒ˜í”Œë§
    TARGET = 32
    if num_total <= TARGET:
        imgs_used = imgs[:]  # ì „ë¶€ ì‚¬ìš©
    else:
        indices = np.linspace(0, num_total - 1, TARGET, dtype=int)
        imgs_used = [imgs[i] for i in indices]

    num_used = len(imgs_used)
    print(f"ğŸ–¼  CLIP: {num_total} frames ì¤‘ ê· ë“± ìƒ˜í”Œë§ {num_used}ê°œ ì‚¬ìš© (target={TARGET})")

    ensure_clip_model()
    per_frame = vc.compute_clip_scores(
        CLIP_MODEL,
        CLIP_PROCESSOR,
        imgs_used,
        CLIP_DEVICE,
        batch_size=batch,
        temperature=2.0,
    )

    if per_frame:
        vals = [v["violence_prob"] for v in per_frame.values()]
        avg_v = float(np.mean(vals))
        max_v = float(np.max(vals))
        p95_v = float(np.percentile(vals, 95))
    else:
        avg_v = max_v = p95_v = 0.0

    result = {
        "model": "openai/clip-vit-base-patch32",
        "frames_dir": frames_dir,
        "num_frames_total": num_total,
        "num_frames_used": num_used,
        "prompts": {
            "harmful": vc.HARMFUL_PROMPTS,
            "benign": vc.BENIGN_PROMPTS,
        },
        "per_frame": per_frame,
        "overall": {
            "avg_violence_prob": avg_v,
            "max_violence_prob": max_v,
            "p95_violence_prob": p95_v,
        },
    }

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(
        f"âœ… CLIP saved -> {out_path} | "
        f"avg={avg_v:.3f}, max={max_v:.3f}, p95={p95_v:.3f}"
    )



def run_vit_inprocess(frames_dir: str, out_path: str, batch: int = 16, stride: int = 10):
    # ğŸ” frames_dir í•˜ìœ„ ì „ì²´ì—ì„œ ì´ë¯¸ì§€ ì¬ê·€ íƒìƒ‰
    pattern = os.path.join(frames_dir, "**", "*")
    imgs = sorted(
        p
        for p in glob(pattern, recursive=True)
        if p.lower().endswith((".jpg", ".jpeg", ".png"))
    )

    num_total = len(imgs)
    if num_total == 0:
        print("âš ï¸ No frames found for ViT violence.")
        result = {
            "model": vv.MODEL_ID,
            "frames_dir": frames_dir,
            "num_frames_total": 0,
            "num_frames_used": 0,
            "per_frame": {},
            "overall": {
                "avg_violence_prob": 0.0,
                "max_violence_prob": 0.0,
                "p95_violence_prob": 0.0,
            },
        }
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        json.dump(result, open(out_path, "w"), indent=2, ensure_ascii=False)
        print(f"âœ… ViT violence saved -> {out_path}")
        return

    # âœ… ê· ë“± 32í”„ë ˆì„ ìƒ˜í”Œë§
    TARGET = 32
    if num_total <= TARGET:
        imgs_used = imgs[:]
    else:
        indices = np.linspace(0, num_total - 1, TARGET, dtype=int)
        imgs_used = [imgs[i] for i in indices]

    num_used = len(imgs_used)
    print(f"ğŸ–¼  ViT Violence: {num_total} frames ì¤‘ ê· ë“± ìƒ˜í”Œë§ {num_used}ê°œ ì‚¬ìš© (target={TARGET})")

    ensure_vit_model()
    per_frame = vv.compute_violence_scores(
        VIT_PROCESSOR,
        VIT_MODEL,
        imgs_used,
        device=VIT_DEVICE,
        batch_size=batch,
    )

    if per_frame:
        vals = [v["violence_prob"] for v in per_frame.values()]
        avg_v = float(np.mean(vals))
        max_v = float(np.max(vals))
        p95_v = float(np.percentile(vals, 95))
    else:
        avg_v = max_v = p95_v = 0.0

    result = {
        "model": vv.MODEL_ID,
        "frames_dir": frames_dir,
        "num_frames_total": num_total,
        "num_frames_used": num_used,
        "per_frame": per_frame,
        "overall": {
            "avg_violence_prob": avg_v,
            "max_violence_prob": max_v,
            "p95_violence_prob": p95_v,
        },
    }

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(
        f"âœ… ViT violence saved -> {out_path} | "
        f"avg={avg_v:.3f}, max={max_v:.3f}, p95={p95_v:.3f}"
    )


def run_slowfast_inprocess(frames_dir: str, out_path: str, frames_per_clip: int = 32, fps: float = 30.0):
    ensure_slowfast_model()

    # ğŸ” frames_dir í•˜ìœ„ì˜ ëª¨ë“  jpgë¥¼ ì¬ê·€ íƒìƒ‰ (full path)
    pattern = os.path.join(frames_dir, "**", "*.jpg")
    all_frames = sorted(glob(pattern, recursive=True))
    num = len(all_frames)
    if num == 0:
        print("âš ï¸ No frames found in:", frames_dir)
        json.dump(
            {
                "model": "slowfast_r101",
                "frames_dir": frames_dir,
                "frames_per_clip": frames_per_clip,
                "clip_sec": float(frames_per_clip / fps) if fps > 0 else 2.0,
                "clips": [],
                "overall": {
                    "num_clips": 0,
                    "avg_top1_prob": 0.0,
                    "max_top1_prob": 0.0,
                    "avg_violence_hint": 0.0,
                    "max_violence_hint": 0.0,
                },
            },
            open(out_path, "w"),
            indent=2,
            ensure_ascii=False,
        )
        print(f"âœ… SlowFast saved -> {out_path}")
        return

    F = frames_per_clip

    # ğŸ¯ ì „ì²´ ì˜ìƒ ê¸¸ì´ (ì´ˆ) ì¶”ì •
    if fps and fps > 0:
        duration = num / fps
        clip_sec = duration  # ì´ 32í”„ë ˆì„ì´ ì „ì²´ ì˜ìƒì„ ëŒ€í‘œí•œë‹¤ê³  ë³´ê³  ì „ì²´ ê¸¸ì´ë¥¼ ì‚¬ìš©
        print(f"[SlowFast] fps={fps} / frames={num} â†’ duration={duration:.4f} sec")
    else:
        duration = 0.0
        clip_sec = float(F / 30.0)  # fallback
        print(f"[SlowFast] Using fallback clip_sec={clip_sec}")

    slowfast = SLOWFAST_MODEL
    labels = SLOWFAST_LABELS
    transform = SLOWFAST_TRANSFORM

    clips_out = []
    top1_list = []
    violence_list = []

    # âœ… ê· ë“± 32í”„ë ˆì„ ìƒ˜í”Œë§
    if num <= F:
        sample_paths = all_frames[:]  # ì „ë¶€ ì‚¬ìš©
    else:
        indices = np.linspace(0, num - 1, F, dtype=int)
        sample_paths = [all_frames[i] for i in indices]

    # í•˜ë‚˜ì˜ clip ìƒì„±
    frames = vsf.load_frames(sample_paths)  # [T,C,H,W]
    frames = torch.stack([transform(fr) for fr in frames])  # [T,C,224,224]

    slow_pathway, fast_pathway = vsf.slowfast_transform(frames)

    slow_pathway = slow_pathway.unsqueeze(0).to(SLOWFAST_DEVICE)
    fast_pathway = fast_pathway.unsqueeze(0).to(SLOWFAST_DEVICE)

    inp = [slow_pathway, fast_pathway]

    with torch.no_grad():
        out = slowfast(inp)

    prob = torch.softmax(out, dim=1)[0]

    top5 = torch.topk(prob, 5)
    top_idx = top5.indices.cpu().tolist()
    top_prob = top5.values.cpu().tolist()

    topk_data = []
    top1_prob = float(top_prob[0])
    top1_list.append(top1_prob)

    violence_hint = 0.0

    for idx, p in zip(top_idx, top_prob):
        label = labels[idx].lower() if idx < len(labels) else f"class_{idx}"
        p_float = float(p)

        topk_data.append({
            "index": idx,
            "label": label,
            "prob": p_float,
        })

        if any(k in label for k in vsf.VIOLENCE_KEYWORDS):
            violence_hint = max(violence_hint, p_float)

    violence_list.append(violence_hint)

    clips_out.append({
        "index": 0,
        "start_sec": 0.0,
        "end_sec": float(clip_sec),
        "topk": topk_data,
        "top1_prob": top1_prob,
        "violence_hint": violence_hint,
    })

    if clips_out:
        avg_top1 = float(sum(top1_list) / len(top1_list))
        max_top1 = float(max(top1_list))
        avg_viol = float(sum(violence_list) / len(violence_list))
        max_viol = float(max(violence_list))
    else:
        avg_top1 = max_top1 = avg_viol = max_viol = 0.0

    out_json = {
        "model": "slowfast_r101",
        "frames_dir": frames_dir,
        "frames_per_clip": frames_per_clip,
        "clip_sec": clip_sec,
        "clips": clips_out,
        "overall": {
            "num_clips": len(clips_out),  # ë³´í†µ 1
            "avg_top1_prob": avg_top1,
            "max_top1_prob": max_top1,
            "avg_violence_hint": avg_viol,
            "max_violence_hint": max_viol,
        },
    }

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    json.dump(out_json, open(out_path, "w"), indent=2, ensure_ascii=False)
    print(f"âœ… SlowFast saved -> {out_path}")



# =========================================
# 1) ìë™ ë¼ë²¨ ìƒì„± (ì‚¬ëŒë³„ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
# =========================================
def auto_generate_video_labels():
    # ìƒëŒ€ê²½ë¡œ( category/clip_001.mp4 ì´ëŸ° ì‹ ) ë¦¬ìŠ¤íŠ¸
    harm_files = get_all_videos(HARM_VIDEO_DIR)
    safe_files = get_all_videos(SAFE_VIDEO_DIR)

    # keyë¥¼ ìƒëŒ€ê²½ë¡œë¡œ ì‚¬ìš© (ë‚˜ì¤‘ì— HARM_VIDEO_DIR/Safeì™€ í•©ì³ì„œ full pathë¡œ ì”€)
    verified_video_labels = {f: 1 for f in harm_files}
    safe_video_labels     = {f: 0 for f in safe_files}

    json.dump(
        verified_video_labels,
        open(os.path.join(LABEL_DIR, "verified_video_labels_init.json"), "w", encoding="utf-8"),
        indent=2,
        ensure_ascii=False,
    )
    json.dump(
        safe_video_labels,
        open(os.path.join(LABEL_DIR, "safe_video_labels_init.json"), "w", encoding="utf-8"),
        indent=2,
        ensure_ascii=False,
    )

    print(f"ğŸ”¹ ë¹„ë””ì˜¤ ìë™ ë¼ë²¨ ìƒì„±: harmful={len(verified_video_labels)}, safe={len(safe_video_labels)}")

    return verified_video_labels, safe_video_labels


# =========================================
# 2) í•œ ì˜ìƒ ì²˜ë¦¬
# =========================================
def process_one_video(video_path, kind: str):
    """
    kind: "harm" ë˜ëŠ” "safe"
    FRAMES_ROOT / (ë¹„ë””ì˜¤ | ì•ˆì „ë¹„ë””ì˜¤) / <ìƒëŒ€ê²½ë¡œ ê¸°ë°˜ ì´ë¦„> ì— í”„ë ˆì„ ì €ì¥
    """
    # ì–´ë–¤ root ê¸°ì¤€ì¸ì§€ ê²°ì •
    if kind == "safe":
        root = SAFE_VIDEO_DIR
        subdir = S_FRAMES_SUBDIR
    else:
        root = HARM_VIDEO_DIR
        subdir = H_FRAMES_SUBDIR

    # root ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ: category1/clip_001.mp4 ì´ëŸ° í˜•íƒœ
    rel = os.path.relpath(video_path, root)
    rel_stem, _ = os.path.splitext(rel)           # category1/clip_001
    safe_stem = rel_stem.replace(os.sep, "__")    # category1__clip_001

    video_name = os.path.basename(video_path)     # ë¡œê·¸ìš©

    frames_dir = os.path.join(FRAMES_ROOT, subdir, safe_stem)
    os.makedirs(frames_dir, exist_ok=True)

    FRAMES_JSON   = os.path.join(frames_dir, "meta.json")
    CLIP_JSON     = os.path.join(frames_dir, "clip_result.json")
    VIT_JSON      = os.path.join(frames_dir, "vit_result.json")
    AUDIO_WAV     = os.path.join(frames_dir, f"{safe_stem}_audio.wav")
    AUDIO_JSON    = os.path.join(frames_dir, "audio_result.json")
    TEXT_JSON     = os.path.join(frames_dir, "text_result.json")
    SLOWFAST_JSON = os.path.join(frames_dir, "slowfast_result.json")
    FUSED_JSON    = os.path.join(frames_dir, "fusion_result.json")

    # 1) ë¹„ë””ì˜¤ â†’ í”„ë ˆì„ split
    run([
        PY_TORCH, VIDEO_SPLIT_PY,
        "--video", video_path,
        "--out", frames_dir,
        "--clip-sec", "2",
    ])

    meta = load_json(FRAMES_JSON, {})
    meta_meta = meta.get("meta", {}) if isinstance(meta.get("meta", {}), dict) else meta

    fps = meta_meta.get("fps", 30.0)
    try:
        fps = float(fps)
    except:
        fps = 30.0

    orig_total_frames = meta_meta.get("orig_total_frames") or meta_meta.get("total_frames_saved") or meta_meta.get("frames") or 0
    try:
        orig_total_frames = int(orig_total_frames)
    except:
        orig_total_frames = 0

    if "duration" in meta_meta:
        try:
            duration = float(meta_meta["duration"])
        except:
            duration = float(orig_total_frames) / fps if fps > 0 and orig_total_frames > 0 else 0.0
    else:
        duration = float(orig_total_frames) / fps if fps > 0 and orig_total_frames > 0 else 0.0

    total_frames = orig_total_frames

    print(f"[{video_name}] fps = {fps}")

    # 2) CLIP
    run_clip_inprocess(
        frames_dir=frames_dir,
        out_path=CLIP_JSON,
        batch=16,
        stride=10,
    )

    # 3) ViT
    run_vit_inprocess(
        frames_dir=frames_dir,
        out_path=VIT_JSON,
        batch=16,
        stride=10,
    )

    # 4) Audio + YAMNet
    audio_ok = False
    if not os.path.exists(AUDIO_WAV):
        cmd = [
            "ffmpeg", "-hide_banner", "-nostdin",
            "-i", video_path, "-ac", "1", "-ar", "16000", "-vn",
            AUDIO_WAV, "-y",
        ]
        print("â–¶", " ".join(cmd))
        p = subprocess.run(cmd)
        if p.returncode == 0 and os.path.exists(AUDIO_WAV):
            audio_ok = True
        else:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ì—†ì–´ì„œ YAMNet ìŠ¤í‚µ: {video_path}")
    else:
        audio_ok = True

    if audio_ok:
        tf_env = os.environ.copy()
        tf_env["CUDA_VISIBLE_DEVICES"] = "-1"
        tf_env["TF_ENABLE_ONEDNN_OPTS"] = "0"
        tf_env["TF_CPP_MIN_LOG_LEVEL"] = "2"

        run([
            PY_TF, AUDIO_YAMNET_PY,
            "--audio", AUDIO_WAV,
            "--out", AUDIO_JSON,
        ], env=tf_env)
    else:
        dummy_audio = {
            "overall": {
                "violent_audio_prob": 0.0,
                "has_audio": False,
            }
        }
        with open(AUDIO_JSON, "w", encoding="utf-8") as f:
            json.dump(dummy_audio, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ dummy audio_result.json ìƒì„± -> {AUDIO_JSON}")

    # 5) TEXT (OCR + Toxic)
    frame_imgs = [
        f for f in os.listdir(frames_dir)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    frame_imgs.sort()

    text_ok = False
    if frame_imgs:
        first_frame = os.path.join(frames_dir, frame_imgs[0])
        try:
            run([
                PY_TORCH,
                TEXT_OCR_PY,
                "--image", first_frame,
                "--out", TEXT_JSON,
            ])
            if os.path.exists(TEXT_JSON):
                text_ok = True
        except RuntimeError:
            print(f"âš ï¸ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨, dummyë¡œ ëŒ€ì²´: {video_path}")
            text_ok = False
    else:
        print(f"âš ï¸ í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ì—†ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ ìŠ¤í‚µ: {video_path}")

    if not text_ok:
        dummy_text = {
            "overall": {
                "hate_prob": 0.0,
                "sexual_text_prob": 0.0,
                "has_text": False,
            }
        }
        with open(TEXT_JSON, "w", encoding="utf-8") as f:
            json.dump(dummy_text, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ dummy text_result.json ìƒì„± -> {TEXT_JSON}")

    # 6) SlowFast
    run_slowfast_inprocess(
        frames_dir=frames_dir,
        out_path=SLOWFAST_JSON,
        frames_per_clip=32,
        fps=fps,
    )

    # 7) Fusion
    run([
        PY_TORCH, FUSION_PY,
        "--clip", CLIP_JSON,
        "--vit", VIT_JSON,
        "--audio", AUDIO_JSON,
        "--text", TEXT_JSON,
        "--slowfast", SLOWFAST_JSON,
        "--out", FUSED_JSON,
    ])

    fused = load_json(FUSED_JSON, {})
    violence_prob = float(
        fused.get("overall", {}).get(
            "violence_prob",
            fused.get("scores", {}).get("final", 0.0),
        )
    )

    # CLIP ì‚¬ìš© í”„ë ˆì„ ìˆ˜
    clip_meta = load_json(CLIP_JSON, {})
    sampled_frames = 0

    if "num_frames_used" in clip_meta:
        try:
            sampled_frames = int(clip_meta["num_frames_used"])
        except:
            sampled_frames = 0

    if sampled_frames == 0 and isinstance(clip_meta.get("per_frame"), dict):
        sampled_frames = len(clip_meta["per_frame"])

    if sampled_frames == 0:
        meta_clip = clip_meta.get("meta", {})
        if isinstance(meta_clip, dict) and "sampled_frames" in meta_clip:
            try:
                sampled_frames = int(meta_clip["sampled_frames"])
            except:
                sampled_frames = 0

    if sampled_frames == 0 and "frames" in clip_meta and isinstance(clip_meta["frames"], list):
        sampled_frames = len(clip_meta["frames"])

    # SlowFast ëŒ€í‘œ í–‰ë™
    slow_json = load_json(SLOWFAST_JSON, {})
    estimated_action = None
    clips = slow_json.get("clips") or []
    if clips:
        labels = []
        for c in clips:
            topk = c.get("topk") or []
            if topk:
                labels.append(topk[0].get("label"))
        if labels:
            estimated_action = Counter(labels).most_common(1)[0][0]
    if not estimated_action:
        estimated_action = "unknown"

    violence_prob = round5(violence_prob)
    print(f"[{video_name}] ğŸ”¥ violence_prob = {violence_prob}")

    video_stats = {
        "duration": duration,
        "fps": fps,
        "total_frames": total_frames,
        "sampled_frames": sampled_frames,
        "estimated_action": estimated_action,
    }

    return violence_prob, video_stats


# =========================================
# 3) ìµœì¢… ë¼ë²¨ ê°±ì‹  (í•˜ë‚˜ì˜ ì‚¬ëŒ ê¸°ì¤€)
# =========================================
def merge_scores_and_update_video_labels(verified, safe):
    TH = 0.63

    # ìœ í•´ ë¹„ë””ì˜¤
    for fname in verified.keys():
        video_path = os.path.join(HARM_VIDEO_DIR, fname)
        score, stats = process_one_video(video_path, kind="harm")
        pred = 1 if score >= TH else 0

        verified[fname] = {
            "duration": stats["duration"],
            "fps": stats["fps"],
            "total_frames": stats["total_frames"],
            "sampled_frames": stats["sampled_frames"],
            "estimated_action": stats["estimated_action"],
            "is_harmful": True,
            "label": 1,
            "pred_label": pred,
            "violence_prob": score,
        }

    # ì•ˆì „ ë¹„ë””ì˜¤
    for fname in safe.keys():
        video_path = os.path.join(SAFE_VIDEO_DIR, fname)
        score, stats = process_one_video(video_path, kind="safe")
        pred = 1 if score >= TH else 0

        category = "safe" if pred == 0 else "review"

        safe[fname] = {
            "is_safe": True,
            "label": 0,
            "pred_label": pred,
            "category": category,
            "duration": stats["duration"],
            "fps": stats["fps"],
            "total_frames": stats["total_frames"],
            "sampled_frames": stats["sampled_frames"],
            "estimated_action": stats["estimated_action"],
            "violence_prob": score,
        }

    json.dump(
        verified,
        open(os.path.join(LABEL_DIR, "verified_video_labels.json"), "w", encoding="utf-8"),
        indent=2,
        ensure_ascii=False,
    )

    json.dump(
        safe,
        open(os.path.join(LABEL_DIR, "safe_video_labels.json"), "w", encoding="utf-8"),
        indent=2,
        ensure_ascii=False,
    )

    print("âœ… ë¹„ë””ì˜¤ ë¼ë²¨ ì €ì¥ ì™„ë£Œ ")


# =========================================
# ì‚¬ëŒ í•œ ëª… ì²˜ë¦¬
# =========================================
def process_person():
    global HARM_VIDEO_DIR, SAFE_VIDEO_DIR, LABEL_DIR, FRAMES_ROOT

    print("\n==============================")
    print(f"ğŸ¬ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘: íŒ€ì›_ë°ì´í„° (100 + 100)")
    print("==============================")

    # ì‚¬ëŒë³„ì´ ì•„ë‹ˆë¼ ê·¸ëŒ€ë¡œ íŒ€ì›_ë°ì´í„° ë£¨íŠ¸ë¥¼ ì‚¬ìš©
    person_src = SRC_ROOT

    # harmful ë¹„ë””ì˜¤ í´ë”: íŒ€ì›_ë°ì´í„°/ë¹„ë””ì˜¤
    harm_dir = os.path.join(person_src, "ë¹„ë””ì˜¤")
    if not os.path.exists(harm_dir):
        raise FileNotFoundError(f"âŒ harmful ë¹„ë””ì˜¤ í´ë” ì—†ìŒ: {harm_dir}")

    # safe ë¹„ë””ì˜¤ í´ë”: íŒ€ì›_ë°ì´í„°/ì•ˆì „_ë¹„ë””ì˜¤
    safe_dir = os.path.join(person_src, "ì•ˆì „_ë¹„ë””ì˜¤")
    if not os.path.exists(safe_dir):
        raise FileNotFoundError(f"âŒ safe ë¹„ë””ì˜¤ í´ë” ì—†ìŒ: {safe_dir}")

    # ì „ì—­ ê°±ì‹ 
    HARM_VIDEO_DIR = harm_dir
    SAFE_VIDEO_DIR = safe_dir

    # ì¶œë ¥ì€ OUT_ROOT/íŒ€ì›_ë°ì´í„°/...
    person_out = os.path.join(OUT_ROOT, "íŒ€ì›_ë°ì´í„°")
    LABEL_DIR = os.path.join(person_out, "ë¼ë²¨_ê²°ê³¼")
    FRAMES_ROOT = os.path.join(person_out, "video_frames")

    os.makedirs(LABEL_DIR, exist_ok=True)
    os.makedirs(FRAMES_ROOT, exist_ok=True)

    print(f"ğŸ“‚ harmful dir: {HARM_VIDEO_DIR}")
    print(f"ğŸ“‚ safe   dir: {SAFE_VIDEO_DIR}")
    print(f"ğŸ“‚ label  dir: {LABEL_DIR}")
    print(f"ğŸ“‚ frames dir: {FRAMES_ROOT}")

    verified, safe = auto_generate_video_labels()
    merge_scores_and_update_video_labels(verified, safe)

    print(f"ğŸ‰ íŒ€ì›_ë°ì´í„° ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ!\n")

# =========================================
# MAIN
# =========================================
def main():
    process_person()
    print("\nğŸ‰ íŒ€ì›_ë°ì´í„° ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()

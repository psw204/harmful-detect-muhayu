# 팀원_전처리 (안지산)

비디오 중심 멀티모달 파이프라인을 위한 **전처리 가이드**입니다.
입력은 `1_공개_데이터셋`의 **RLVS / RWF-2000** (또는 `2_실제_수집_데이터(개인)`)의 원본 **mp4**이며,
출력은 `4_전처리_결과(개인)/안지산/팀원_전처리/` 하위에 **클립(mp4) · 오디오(wav) · 텍스트(txt) · 매니페스트(jsonl)**로 생성됩니다.

---

## 📁 폴더 구조

```
무하유_유해콘텐츠_데이터/
 ├─ 1_공개_데이터셋/
 │   ├─ RLVS/
 │   └─ RWF-2000/
 ├─ 2_실제_수집_데이터(개인)/
 └─ 4_전처리_결과(개인)/
     └─ 안지산/
         └─ 팀원_전처리/
             ├─ clips/           # 클립 mp4 (윈도우 단위)
             ├─ audio/           # 16kHz mono wav (클립 구간)
             ├─ text/            # srt/ASR에서 추출된 구간 텍스트
             └─ manifests/
                 └─ multimodal.jsonl  # 클립별 메타/경로 모음
```

---

## ⚙️ 요구 사항

* 시스템: `ffmpeg` 설치 필요 (리눅스/맥/윈도우 공통)
* 파이썬 패키지: `pip install opencv-python numpy tqdm srt`
* (선택) ASR 사용 시: Whisper 등 외부 도구를 사용하도록 훅 제공 — 기본은 **SRT 우선**, 없으면 **텍스트 비우기**

---

## 💡 핵심 아이디어

* **하나의 비디오**에서 **클립(예: 4초)** 단위로 자르고, 해당 구간의 **오디오**와 **텍스트**를 **동일 타임스탬프**로 동기화해 저장합니다.
* 각 클립은 고유한 `clip_id`를 가지며, 모든 경로와 메타는 `manifests/multimodal.jsonl`에 **한 줄 JSON**으로 누적됩니다.
* 라벨은 이 단계에서 비워두고, 라벨링 툴에서 `clip_id` 기준으로 부여합니다.

---

## 🧭 실행 예시

프로젝트 루트에서:

```bash
python 팀원_전처리/preprocess_videos.py \
  --member 안지산 \
  --public-root 무하유_유해콘텐츠_데이터/1_공개_데이터셋 \
  --out-root 무하유_유해콘텐츠_데이터/4_전처리_결과(개인)/안지산/팀원_전처리 \
  --datasets RLVS RWF-2000 \
  --win 4.0 --stride 2.0 --resume
```

**옵션 요약**

| 옵션                   | 설명                                |
| -------------------- | --------------------------------- |
| `--member`           | 산출물 메타에 기록할 팀원명                   |
| `--public-root`      | 공개데이터셋 루트 경로                      |
| `--datasets`         | 처리할 데이터셋 이름들 (기본: RLVS, RWF-2000) |
| `--out-root`         | 전처리 결과 출력 루트                      |
| `--win` / `--stride` | 클립 길이(초), 슬라이딩 보폭(초)              |
| `--min-dur`          | 최소 영상 길이 제한                       |
| `--video-limit`      | 최대 처리 개수                          |
| `--resume`           | 이미 생성된 산출물은 건너뛰기                  |

---

## 🧾 매니페스트 스키마 (JSONL)

```json
{
  "clip_id": "RWF-2000/train/violence/vid001_000.0_004.0",
  "member": "안지산",
  "video": {"src": "...", "clip_path": "...", "start": 0.0, "end": 4.0, "fps": 30.0, "duration": 4.0},
  "audio": {"path": "...", "sr": 16000, "channels": 1},
  "text": {"path": "...", "source": "srt|none", "content_preview": "(구간 자막 일부)"},
  "labels": {"action": null, "audio": null, "text": null}
}
```

---

## 🧩 라벨링 연계

* `clip_id` 기준으로 라벨링 툴이 `labels.action / labels.audio / labels.text`를 채움.
* 라벨링 결과는 `3_라벨링_파일(개인)/안지산/라벨_결과/`에 저장.

---

## ✅ 체크리스트

* [ ] ffmpeg 설치 (`ffmpeg -version`)
* [ ] 패키지 설치 (`pip install opencv-python numpy tqdm srt`)
* [ ] RLVS, RWF-2000 mp4 존재 확인
* [ ] 실행 후 `clips/`, `audio/`, `text/`, `manifests/multimodal.jsonl` 생성 확인

---

## 🚀 다음 단계

전처리 후 생성된 `multimodal.jsonl`을 기준으로 라벨링 툴(`team_labeling_tool.py`)에서
행동(action) / 오디오(audio) / 텍스트(text) 라벨을 부여해 학습용 데이터를 완성합니다.

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import argparse
from glob import glob

os.environ["TRANSFORMERS_NO_TF"] = "1"

import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import numpy as np

# -----------------------------
# 1. í”„ë¡¬í”„íŠ¸: harmful / benign ì§ ë§žì¶”ê¸°
# -----------------------------
HARMFUL_PROMPTS = [
    "a violent scene with people fighting, punching or kicking",
    "a person shooting a gun at another person",
    "visible blood, gore or serious injury",
    "a person holding a weapon in a threatening or aggressive way",
    "a brutal fight scene from an action movie",
    "an explicit violent scene that should not be shown to children",
    
]

BENIGN_PROMPTS = [
    "people calmly talking with no fighting or violence",
    "a person holding a harmless everyday object, no threat",
    "no blood or injury, just normal healthy people",
    "a person holding tools or everyday items in a safe way",
    "a normal peaceful scene with people standing or walking",
    "a safe and non-violent scene that is appropriate for all ages",
]


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--frames", required=True, help="í”„ë ˆìž„ ì´ë¯¸ì§€ë“¤ì´ ìžˆëŠ” ë””ë ‰í† ë¦¬")
    ap.add_argument("--out", required=True, help="ì¶œë ¥ JSON ê²½ë¡œ")
    ap.add_argument("--batch", type=int, default=16, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 16)")
    ap.add_argument(
        "--stride",
        type=int,
        default=10,
        help="í”„ë ˆìž„ ìƒ˜í”Œë§ ê°„ê²© (Nìž¥ ì¤‘ 1ìž¥ë§Œ ì‚¬ìš©, ê¸°ë³¸ 10)",
    )
    ap.add_argument(
        "--device",
        default=None,
        help="cuda:0 ë˜ëŠ” cpu (ê¸°ë³¸: cuda ê°€ëŠ¥í•˜ë©´ cuda:0, ì•„ë‹ˆë©´ cpu)",
    )
    ap.add_argument(
        "--temperature",
        type=float,
        default=2.0,
        help="softmax temperature (ê°’ì´ í´ìˆ˜ë¡ ë¶„í¬ê°€ í‰í‰í•´ì§, ê¸°ë³¸ 2.0)",
    )
    # 0.85 ê¸°ì¤€/ìƒ¤í”„ë‹ˆìŠ¤ë„ CLIì—ì„œ ë°”ê¿€ ìˆ˜ ìžˆê²Œ í•´ë„ ë˜ì§€ë§Œ,
    # ì§€ê¸ˆì€ ì½”ë“œ ì•ˆì— ìƒìˆ˜ë¡œ ë‘ê³  íŠœë‹í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‘ .
    return ap.parse_args()


def load_model(device):
    print("ðŸ” Loading CLIP model (openai/clip-vit-base-patch32)...")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    model.to(device)
    model.eval()
    return model, processor


# -----------------------------
# 2. 0.85 ê¸°ì¤€ ë¦¬ìŠ¤ì¼€ì¼ í•¨ìˆ˜
# -----------------------------
# def rescale_clip(p: float, center: float = 0.85, sharpness: float = 25.0) -> float:
#     """
#     p: 0~1 ì‚¬ì´ì˜ ì›ë³¸ CLIP harmful í•© ì ìˆ˜
#     center: ê¸°ì¤€ì  (ì˜ˆ: 0.85 -> ì—¬ê¸°ì„œ 0.5ê°€ ë‚˜ì˜¤ë„ë¡)
#     sharpness: ê¸°ìš¸ê¸°. í´ìˆ˜ë¡ center ì£¼ë³€ì—ì„œ ê¸‰ê²©ížˆ 0/1ë¡œ ë‚˜ë‰¨
#     """
#     x = p - center        # centerë³´ë‹¤ í¬ë©´ ì–‘ìˆ˜, ìž‘ìœ¼ë©´ ìŒìˆ˜
#     y = 1.0 / (1.0 + np.exp(-sharpness * x))  # ì‹œê·¸ëª¨ì´ë“œ
#     return float(np.clip(y, 0.0, 1.0))


# -----------------------------
# 3. CLIP ì ìˆ˜ ê³„ì‚° ë¡œì§
# -----------------------------
@torch.no_grad()
def compute_clip_scores(
    model,
    processor,
    image_paths,
    device,
    batch_size: int = 16,
    temperature: float = 2.0,
):
    """
    image_paths: ë¦¬ìŠ¤íŠ¸[str]
    ë°˜í™˜: dict {filename: violence_prob(float)}

    - harmful + benign í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ë„£ê³  softmax
    - harmful í”„ë¡¬í”„íŠ¸ë“¤ í™•ë¥ ì„ í•©ì‚°í•´ì„œ harm_prob ê³„ì‚°
    - harm_probë¥¼ 0.85 ê¸°ì¤€ìœ¼ë¡œ 0~1 ìŠ¤ì¼€ì¼ë¡œ ë‹¤ì‹œ ë§¤í•‘
    """
    texts = HARMFUL_PROMPTS + BENIGN_PROMPTS
    num_harm = len(HARMFUL_PROMPTS)

    per_frame = {}

    for i in range(0, len(image_paths), batch_size):
        chunk = image_paths[i: i + batch_size]
        images = []
        valid_paths = []
        for p in chunk:
            try:
                img = Image.open(p).convert("RGB")
                images.append(img)
                valid_paths.append(p)
            except Exception:
                # ê¹¨ì§„ ì´ë¯¸ì§€ ë“±ì€ ìŠ¤í‚µ
                continue

        if not images:
            continue

        inputs = processor(
            text=texts,
            images=images,
            return_tensors="pt",
            padding=True,
        ).to(device)

        out = model(**inputs)
        logits = out.logits_per_image  # (B, T)

        # temperature scalingìœ¼ë¡œ ë¶„í¬ë¥¼ ëœ ë¾°ì¡±í•˜ê²Œ
        logits = logits / temperature
        probs = logits.softmax(dim=-1).cpu().numpy()  # softmax over text

        for path, prob_vec in zip(valid_paths, probs):
            # 1) harmful í”„ë¡¬í”„íŠ¸ ìª½ í™•ë¥  í•©ì‚°
            harm_prob = float(np.sum(prob_vec[:num_harm]))
            harm_prob = float(np.clip(harm_prob, 0.0, 1.0))

            # 2) 0.85 ê¸°ì¤€ìœ¼ë¡œ 0~1 ë‹¤ì‹œ ë§¤í•‘
            #harm_score = rescale_clip(harm_prob, center=0.85, sharpness=25.0)

            fname = os.path.basename(path)
            per_frame[fname] = {"violence_prob": harm_prob}

    return per_frame


# -----------------------------
# 4. ë©”ì¸
# -----------------------------
if __name__ == "__main__":
    args = parse_args()

    device = args.device
    if device is None:
        device = "cuda:0" if torch.cuda.is_available() else "cpu"

    frames_dir = args.frames
    out_path = args.out

    imgs = sorted(
        [
            p
            for p in glob(os.path.join(frames_dir, "*"))
            if p.lower().endswith((".jpg", ".jpeg", ".png"))
        ]
    )

    num_total = len(imgs)
    if num_total == 0:
        print("âš ï¸ No frames found for CLIP.")
        result = {
            "model": "openai/clip-vit-base-patch32",
            "frames_dir": frames_dir,
            "num_frames_total": 0,
            "num_frames_used": 0,
            "prompts": {
                "harmful": HARMFUL_PROMPTS,
                "benign": BENIGN_PROMPTS,
            },
            "per_frame": {},
            "overall": {
                "avg_violence_prob": 0.0,
                "max_violence_prob": 0.0,
                "p95_violence_prob": 0.0,
            },
        }
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        json.dump(result, open(out_path, "w"), indent=2, ensure_ascii=False)
        print(f"âœ… CLIP saved -> {out_path}")
        exit(0)

    # stride ì ìš©í•´ì„œ ì¼ë¶€ í”„ë ˆìž„ë§Œ ì‚¬ìš©
    imgs_used = imgs[:: max(1, args.stride)]
    num_used = len(imgs_used)
    print(f"ðŸ–¼  CLIP: {num_total} frames ì¤‘ {num_used}ê°œ ì‚¬ìš© (stride={args.stride})")

    model, processor = load_model(device)
    per_frame = compute_clip_scores(
        model,
        processor,
        imgs_used,
        device,
        batch_size=args.batch,
        temperature=args.temperature,
    )

    if per_frame:
        vals = [v["violence_prob"] for v in per_frame.values()]
        avg_v = float(np.mean(vals))
        max_v = float(np.max(vals))
        p95_v = float(np.percentile(vals, 95))
    else:
        avg_v = max_v = p95_v = 0.0

    result = {
        "model": "openai/clip-vit-base-patch32",
        "frames_dir": frames_dir,
        "num_frames_total": num_total,
        "num_frames_used": num_used,
        "prompts": {
            "harmful": HARMFUL_PROMPTS,
            "benign": BENIGN_PROMPTS,
        },
        "per_frame": per_frame,
        "overall": {
            "avg_violence_prob": avg_v,
            "max_violence_prob": max_v,
            "p95_violence_prob": p95_v,
        },
    }

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(
        f"âœ… CLIP saved -> {out_path} | "
        f"avg={avg_v:.3f}, max={max_v:.3f}, p95={p95_v:.3f}"
    )
